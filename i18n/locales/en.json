{
  "nav": {
    "projects": "Projects",
    "stack": "Stack",
    "about": "About",
    "backToPortfolio": "Back to Portfolio",
    "back": "Voltar"
  },
  "hero": {
    "title": "Marcelo Marleta",
    "subtitle": "Senior Backend & AI Engineer",
    "description": "Enterprise conversation architectures with LangGraph, optimized RAG and multi-tenant systems. Focus on separation of deterministic logic and natural language.",
    "github": "GitHub",
    "linkedin": "LinkedIn"
  },
  "projects": {
    "title": "Projects",
    "description": "Production systems focused on conversational AI, document processing and development tools.",
    "optimusPlatform": "Optimus Platform",
    "otherProjects": "Other Projects",
    "viewMore": "View All Projects"
  },
  "stack": {
    "title": "Stack",
    "backend": "Backend",
    "ai": "AI/ML",
    "infra": "Infra"
  },
  "cta": {
    "wantToSeeMore": "Want to see more projects?",
    "exploreOther": "Also explore other case studies in my portfolio."
  },
  "common": {
    "fullStack": "Full-Stack Platform",
    "realTime": "Real-time",
    "multiPlatform": "Multi-Platform",
    "back": "<- Back",
    "technicalStack": "Technical Stack",
    "next": "Next",
    "personalProject": "Personal Project",
    "exploreOtherProjects": "Explore Other Projects",
    "seeOtherSystems": "See other systems I built",
    "viewAllProjects": "View All Projects"
  },
  "cards": {
    "aiEngine": {
      "title": "AI Conversation Engine",
      "description": "LangGraph + FSM for separation of deterministic logic and natural language. 20+ specialized nodes, 8 booking states."
    },
    "llmPool": {
      "title": "LLM Pool Management",
      "description": "Separate pools for chat vs tools, key groups with encrypted vault, automatic rotation and intelligent fallback. 40% cost reduction."
    },
    "pricing": {
      "title": "Pricing Intelligence",
      "description": "Optimized RAG with adaptive threshold, request coalescing and multi-tier cache. 95% precision, <100ms cache hit."
    },
    "memory": {
      "title": "Memory Engine",
      "description": "Hierarchical Hot/Warm/Cold system with semantic compression (90% reduction), automatic LGPD/HIPAA compliance and context injection."
    },
    "backend": {
      "title": "Backend Orchestrator",
      "description": "Central FastAPI hub with fail-closed design, tenant isolation, agent handover, circuit breakers."
    },
    "rules": {
      "title": "Rules Engine",
      "description": "Python Lambda DSL for business rules, fast-lane patterns, backend + AI engine coordination."
    },
    "whatsapp": {
      "title": "WhatsApp Integration",
      "description": "Isolated Redis, webhook processing, WebSocket broadcaster, Memory Engine integration for history."
    },
    "audio": {
      "title": "Audio Processor",
      "description": "Multi-provider STT/TTS (Whisper, ElevenLabs, Azure), async processing with Celery, various formats."
    },
    "observability": {
      "title": "Observability",
      "description": "Prometheus + OpenTelemetry + structlog. Distributed tracing, custom metrics, dashboards."
    },
    "frontend": {
      "title": "Frontend Apps",
      "description": "Admin Dashboard (3000) + SuperAdmin Panel (3001). Vue.js 3, real-time WebSocket, handover management, RBAC."
    },
    "testTools": {
      "title": "AI Testing Tools",
      "description": "Hallucination detector, quality scorer, realistic scenarios with personalities, multi-channel simulation interface."
    },
    "infraDevops": {
      "title": "Infra & DevOps",
      "description": "Docker Compose overlays, Nginx LB with horizontal scaling, Redis Sentinel HA, CI/CD with architecture guardrails."
    },
    "architecture": {
      "title": "Complete Architecture",
      "description": "Platform overview: 4 microservices, multi-tenant, WhatsApp integration, hierarchical Memory Engine."
    },
    "mcp": {
      "title": "MCP Servers",
      "description": "Model Context Protocol servers for Claude Code. Debugging, architectural validation and codebase navigation."
    },
    "automark": {
      "title": "AutoMark Platform",
      "description": "Multi-tenant SaaS for affiliate offer distribution. Shopee ‚Üí WhatsApp with anti-spam, scoring and dedupe."
    },
    "icontei": {
      "title": "iContei",
      "description": "Social network for shareable counters. Next.js 16, FastAPI, real-time WebSocket, AI verification, Redis rankings."
    },
    "gratidiem": {
      "title": "GratiDiem",
      "description": "Flutter gratitude app with 508+ files, 6 platforms. Riverpod, dual persistence (Hive+Firebase), AI with Gemini."
    },
    "pvcoach": {
      "title": "PVCoach",
      "description": "Chess coach with Stockfish + LLM. MultiPV analysis, grounded explanations, progressive hints."
    },
    "feedRss": {
      "title": "Feed-RSS Monitor",
      "description": "Content automation pipeline: RSS ‚Üí Filter ‚Üí OpenAI ‚Üí Telegram/Discord. Auto-generated Shorts scripts."
    }
  },
  "cases": {
    "automark": {
      "meta": {
        "title": "AutoMark - Affiliate Marketing Platform | Marcelo Marleta",
        "description": "Case study: Multi-tenant SaaS platform for intelligent affiliate offer distribution. Shopee, WhatsApp, anti-spam, intelligent scoring."
      },
      "header": {
        "back": "Back",
        "projectType": "Personal Project"
      },
      "hero": {
        "tag1": "SaaS Platform",
        "tag2": "Affiliate Marketing",
        "title": "AutoMark",
        "titleHighlight": " Platform",
        "description": "Multi-tenant SaaS platform for intelligent affiliate offer distribution. Connects marketplaces (Shopee, ML, Amazon) to channels (WhatsApp, Telegram) with anti-spam automations, intelligent scoring and deduplication.",
        "stat1Value": "Multi",
        "stat1Label": "Marketplace",
        "stat2Value": "Multi",
        "stat2Label": "Channel",
        "stat3Value": "48h",
        "stat3Label": "Dedupe Target",
        "stat4Value": "0",
        "stat4Label": "Spam/Ban"
      },
      "nav": {
        "problem": "Problem",
        "architecture": "Architecture",
        "providers": "Providers",
        "scoring": "Scoring",
        "dedupe": "Dedupe",
        "antiBan": "Anti-Ban",
        "automations": "Automations",
        "stack": "Stack"
      },
      "problem": {
        "title": "The Problem",
        "intro": "Affiliates face three critical problems in offer distribution:",
        "issue1Title": "Manual distribution doesn't scale:",
        "issue1Desc": "Posting links manually in groups leads to repetition, spam and bans. Time spent vs conversion doesn't pay off.",
        "issue2Title": "Marketplaces are fragmented:",
        "issue2Desc": "Shopee, Mercado Livre, Amazon have different APIs, different formats, different affiliate rules.",
        "issue3Title": "WhatsApp punishes robotic behavior:",
        "issue3Desc": "Without cadence control, dedupe and humanization: shadow ban, blocking, engagement drop.",
        "objectiveTitle": "üéØ Objective",
        "objectiveDesc": "Create a platform that looks human, posts what converts, in the right place, at the right time, without spam."
      },
      "architecture": {
        "title": "Multi-Tenant Architecture",
        "intro": "The architecture follows strict principles that allow evolution without breaking changes:",
        "principle1Title": "Marketplace never hardcoded",
        "principle1Desc": "Shopee is the first connector, but ML, Amazon, AliExpress are plug-and-play.",
        "principle2Title": "Channel never hardcoded",
        "principle2Desc": "WhatsApp is the first, but Telegram, Discord, Instagram follow the same interface.",
        "principle3Title": "Automation generates Post, doesn't send",
        "principle3Desc": "Separation of concerns: Automation decides WHAT, Dispatcher decides HOW.",
        "principle4Title": "Mandatory dedupe",
        "principle4Desc": "No send happens without passing through dedupe. Zero spam guaranteed.",
        "dataModelTitle": "Data Model",
        "rolesTitle": "Roles and Permissions",
        "superAdminTitle": "SuperAdmin (Platform)",
        "superAdminRole1": "Creates tenants",
        "superAdminRole2": "Defines which marketplaces exist",
        "superAdminRole3": "Controls feature flags",
        "superAdminRole4": "Observes global health",
        "tenantAdminTitle": "Tenant Admin (Client)",
        "tenantAdminRole1": "Connects channels (WhatsApp, Telegram)",
        "tenantAdminRole2": "Connects affiliates (Shopee, ML)",
        "tenantAdminRole3": "Creates automations",
        "tenantAdminRole4": "Views history and metrics"
      },
      "providers": {
        "title": "Provider Pattern",
        "intro": "Each marketplace implements a common interface. The provider doesn't know what tenant is ‚Äî everything comes via tenant_connection. This allows adding new marketplaces without touching the core.",
        "shopeeTitle": "Shopee Provider",
        "dedupeKeyTitle": "‚úì Dedupe Key",
        "dedupeKeyDesc": "The dedupe_key is the product's real identity: shopee:123:456. Never use title for dedupe ‚Äî text variations would cause spam."
      },
      "scoring": {
        "title": "Offer Scoring",
        "intro": "Not every offer is worth posting. The scoring system ranks offers by conversion potential, penalizing suspicious products.",
        "formulaTitle": "Score Formula",
        "weightsTitle": "Why these weights?",
        "weight1Pct": "40%",
        "weight1Label": "Discount:",
        "weight1Desc": "Main click driver. High discount offers convert more.",
        "weight2Pct": "25%",
        "weight2Label": "Rating:",
        "weight2Desc": "Product trust. Low rating = return = canceled commission.",
        "weight3Pct": "20%",
        "weight3Label": "Sales:",
        "weight3Desc": "Social proof. High-selling product has market validation.",
        "weight4Pct": "15%",
        "weight4Label": "Commission:",
        "weight4Desc": "Return for affiliate. But too high commission is a red flag."
      },
      "dedupe": {
        "title": "Anti-Spam Deduplication",
        "intro": "The dedupe system operates in two scopes: by target (specific group) and by channel (all channel groups). This prevents both vertical and horizontal spam.",
        "targetScopeTitle": "Target Scope (48h)",
        "targetScopeDesc": "Same product can't be sent to the same group in 48h. Avoids perceived repetition by members.",
        "channelScopeTitle": "Channel Scope (2-5min)",
        "channelScopeDesc": "Minimum gap between any send to any group. Avoids bot-like bursts.",
        "cleanupTitle": "‚úì Automatic Cleanup",
        "cleanupDesc": "Expired records are periodically deleted via cleanup_expired(). The table doesn't grow indefinitely."
      },
      "antiBan": {
        "title": "Anti-Ban (WhatsApp)",
        "intro": "WhatsApp detects bots by sending patterns. The system implements multiple humanization layers to avoid banning:",
        "feature1Icon": "üñäÔ∏è",
        "feature1Title": "Typing Indicator:",
        "feature1Desc": "Before sending, simulates \"typing...\" for 2-5 seconds (Evolution API).",
        "feature2Icon": "üé≤",
        "feature2Title": "Human Jitter:",
        "feature2Desc": "Random delay between messages. Never exact intervals.",
        "feature3Icon": "üö¶",
        "feature3Title": "Per-Account Rate Limit:",
        "feature3Desc": "Redis semaphore limits msgs/minute per number. Multiple accounts = more throughput.",
        "feature4Icon": "‚ö°",
        "feature4Title": "Circuit Breaker:",
        "feature4Desc": "429 or 503 error ‚Üí automatic 30min pause for that account.",
        "feature5Icon": "üåô",
        "feature5Title": "Quiet Hours:",
        "feature5Desc": "Configurable per target: doesn't send between 11pm-8am (or custom)."
      },
      "automations": {
        "title": "Automations System",
        "intro": "Automations are the heart of the system. They define WHAT to search, WHERE to send, and WHEN to execute.",
        "pipelineTitle": "Automation Pipeline",
        "typesTitle": "Automation Types",
        "typeSearchTitle": "search",
        "typeSearchDesc": "Active marketplace search. Executes at each configured interval.",
        "typeFeedTitle": "feed",
        "typeFeedDesc": "Uses offers already in database (external ingestion). Applies ranking and distributes.",
        "typeMonitorTitle": "monitor",
        "typeMonitorDesc": "Monitors specific product prices. Alerts when discount reaches threshold.",
        "targetTitle": "AutomationTarget (Limits)",
        "targetConfigTitle": "Per-Group Configuration"
      },
      "stack": {
        "title": "Tech Stack",
        "backendTitle": "Backend",
        "frontendTitle": "Frontend",
        "integrationsTitle": "Integrations",
        "infraTitle": "Infra"
      },
      "results": {
        "title": "Results",
        "stat1Value": "0",
        "stat1Label": "WhatsApp account bans",
        "stat2Value": "100%",
        "stat2Label": "Dedupe coverage (zero spam)",
        "stat3Value": "Plug",
        "stat3Label": "& Play for new marketplaces",
        "stat4Value": "Multi",
        "stat4Label": "Tenant from day 1"
      },
      "cta": {
        "title": "Explore Other Projects",
        "description": "See other systems I built"
      },
      "footer": {
        "caseStudy": "Case Study: AutoMark Platform ‚Äî Affiliate Marketing Automation"
      }
    },
    "icontei": {
      "meta": {
        "title": "iContei - Social Network for Counters | Marcelo Marleta",
        "description": "Case study: Full-stack platform for shareable counters with Next.js 16, FastAPI, real-time WebSocket, and AI verification."
      },
      "hero": {
        "tag1": "Full-Stack Platform",
        "tag2": "Social Network",
        "tag3": "Real-time",
        "title": "iContei",
        "description": "Social network for shareable counters ‚Äî from viral sports statistics to personal milestones. Full-stack platform with Next.js 16, FastAPI, WebSockets, and AI verification."
      },
      "overview": {
        "title": "Overview",
        "productTitle": "The Product",
        "productDesc1": "iContei is a social platform dedicated to counters of all types ‚Äî from viral sports statistics (\"It's been 2,847 days since Flamengo's last world title\") to personal milestones (\"127 days until my wedding\") and corporate ones (\"1,567 days without workplace accidents\").",
        "productDesc2": "The focus is creating visually impactful counters for social media sharing (TikTok, Instagram, Twitter) with dynamic previews, real-time updates, and engaged community.",
        "useCasesTitle": "Use Cases",
        "publicTitle": "üìä Public (Viral)",
        "publicExample1": "\"It's been 2,847 days since Flamengo's last world title\"",
        "publicExample2": "\"180 days until World Cup 2026\"",
        "publicExample3": "\"Real Madrid 45 days unbeaten at Bernab√©u\"",
        "personalTitle": "üíë Personal",
        "personalExample1": "\"127 days until my wedding\"",
        "personalExample2": "\"1,234 days since we met\"",
        "corporateTitle": "üè¢ Corporate",
        "corporateExample1": "\"1,567 days without workplace accidents\"",
        "corporateExample2": "\"890 days as market leader\""
      },
      "architecture": {
        "title": "Full-Stack Architecture",
        "frontendStackTitle": "Frontend Stack",
        "backendStackTitle": "Backend Stack"
      },
      "realtime": {
        "title": "Real-time System",
        "description": "Counters update every second on the frontend. To support thousands of simultaneous connections, I implemented a WebSocket system with Redis pub/sub, heartbeat, and intelligent rate limiting.",
        "websocketTitle": "WebSocket Manager",
        "websocketDesc": "Dual rate limiting: by authenticated user (5 connections) and by IP for anonymous (20 connections, considering NAT/CGNAT). Heartbeat via Redis Sorted Set to detect dead connections.",
        "pubsubTitle": "Redis Pub/Sub Distribution",
        "pubsubDesc": "With Redis pub/sub, I can horizontally scale the backend ‚Äî each instance maintains its WebSocket connections and receives updates via Redis.",
        "presenceTitle": "Presence System",
        "presenceStat1Value": "30s",
        "presenceStat1Label": "Heartbeat interval",
        "presenceStat2Value": "45s",
        "presenceStat2Label": "Presence timeout",
        "presenceStat3Value": "ZADD",
        "presenceStat3Label": "Redis Sorted Set",
        "presenceDesc": "Client sends heartbeat every 30s. Server uses ZRANGEBYSCORE to list active connections (timestamp > now - 45s). Stale connections are automatically removed."
      },
      "aiVerification": {
        "title": "AI Verification",
        "description": "Official counters go through automated verification with AI to ensure dates and information are correct. Multi-provider system with fallback.",
        "serviceTitle": "AI Verification Service",
        "pipelineTitle": "Brave Search + Groq Pipeline",
        "pipelineDesc": "For counters that need external data (e.g.: \"team X's last win\"), I use Brave Search to find recent information and Groq to extract structured data."
      },
      "rankings": {
        "title": "Rankings System",
        "description": "Real-time rankings using Redis Sorted Sets with 60/30/10 scoring algorithm that balances likes, comments and views.",
        "algorithmTitle": "Scoring Algorithm",
        "redisTitle": "Redis Sorted Sets",
        "redisDesc": "ZADD to insert, ZREVRANGE to fetch top N, ZREMRANGEBYRANK to maintain controlled size. O(log N) for all operations."
      },
      "automation": {
        "title": "Automation System",
        "description": "Counters can be automatically updated via configurable rules ‚Äî from cron schedules to triggers based on external APIs.",
        "scheduledTitle": "Scheduled",
        "scheduledDesc": "Cron expressions for periodic updates. Uses croniter to calculate next execution.",
        "apiTriggerTitle": "API Trigger",
        "apiTriggerDesc": "Integration with external APIs (API-Football, etc.) to automatically fetch data.",
        "crowdsourcedTitle": "Crowdsourced",
        "crowdsourcedDesc": "Users can suggest updates that go through approval before being applied.",
        "sportsProviderTitle": "Sports API Provider",
        "sportsProviderDesc": "Provider Pattern allows easily adding new data sources. Each provider implements fetch_data() and returns standardized response."
      },
      "observability": {
        "title": "Observability",
        "description": "Complete observability stack to monitor the platform in production.",
        "prometheusTitle": "Prometheus + Metrics",
        "otelTitle": "OpenTelemetry",
        "loggingTitle": "Structured Logging"
      },
      "testing": {
        "title": "Automated Testing",
        "description": "Complete coverage with pytest (backend) and vitest + Playwright (frontend).",
        "backendTitle": "Backend (pytest)",
        "backendTestCount": "267",
        "backendTestLabel": "tests",
        "backendStatus": "‚úì",
        "backendStatusLabel": "passing",
        "frontendTitle": "Frontend (vitest + Playwright)",
        "frontendUnitCount": "21",
        "frontendUnitLabel": "unit tests",
        "frontendE2E": "E2E",
        "frontendE2ELabel": "Playwright"
      },
      "workers": {
        "title": "Background Workers",
        "description": "Asynchronous processing for heavy tasks, keeping the API responsive.",
        "schedulerTitle": "Automation Scheduler",
        "schedulerDesc": "Executes automation rules every 5 minutes. Calculates next run with croniter.",
        "rankingTitle": "Ranking Worker",
        "rankingDesc": "Updates Redis Sorted Sets with calculated scores for each period (24h, 7d, 30d, all).",
        "reviewTitle": "Review Worker",
        "reviewDesc": "Processes counter verification queues, coordinates with AI Verification Service.",
        "monitoringTitle": "AI Monitoring",
        "monitoringDesc": "Monitors counters marked for continuous verification, updates when data changes."
      },
      "roadmap": {
        "title": "Roadmap",
        "phase1Title": "Phase 1 - MVP ‚úÖ",
        "phase1Item1": "‚úì Counter CRUD",
        "phase1Item2": "‚úì Theme system",
        "phase1Item3": "‚úì Rankings (Trending)",
        "phase1Item4": "‚úì Dynamic OG images",
        "phase1Item5": "‚úì Authentication",
        "phase1Item6": "‚úì Follow + reactions",
        "phase2Title": "Phase 2 - Social",
        "phase2Item1": "Personalized feed",
        "phase2Item2": "Comments and discussions",
        "phase2Item3": "Push notifications",
        "phase2Item4": "Badges and gamification",
        "phase2Item5": "Robust public profiles",
        "phase3Title": "Phase 3 - Enterprise",
        "phase3Item1": "Corporate profiles",
        "phase3Item2": "Multi-user per org",
        "phase3Item3": "Analytics dashboards",
        "phase3Item4": "Corporate SSO",
        "phase3Item5": "Licensed themes",
        "phase4Title": "Phase 4 - Creators",
        "phase4Item1": "Video generation (Remotion)",
        "phase4Item2": "Social media SDK",
        "phase4Item3": "Public API",
        "phase4Item4": "Embeddable widgets",
        "phase4Item5": "Template marketplace"
      },
      "stackSection": {
        "title": "Complete Stack"
      },
      "cta": {
        "title": "Want to see more projects?",
        "description": "This is one of my personal projects. Also explore Optimus (conversational AI platform) and other case studies in my portfolio."
      }
    },
    "gratidiem": {
      "meta": {
        "title": "GratiDiem - Flutter Gratitude App | Marcelo Marleta",
        "description": "Case study: Multi-platform Flutter gratitude app with Clean Architecture, Riverpod, Firebase, AI integrated with Gemini, and ethical monetization system."
      },
      "hero": {
        "tag1": "Flutter App",
        "tag2": "Multi-Platform",
        "tag3": "Firebase",
        "title": "GratiDiem",
        "description": "Gratitude and wellness application with 508+ Dart files, 35+ screens, 86+ services. Clean Architecture with Riverpod, dual persistence (Hive + Firebase), AI integrated with Gemini, and gamified ethical monetization system."
      },
      "overview": {
        "title": "Overview",
        "productTitle": "The Product",
        "productDesc1": "GratiDiem is a daily gratitude companion ‚Äî a sophisticated application that helps users cultivate wellness through gratitude practices, guided meditation, daily challenges, and community.",
        "productDesc2": "The app supports 6 platforms (Android, iOS, Web, macOS, Windows, Linux), 3 languages (PT, EN, ES), and is built to scale to millions of users.",
        "metricsTitle": "Codebase Metrics",
        "metricFilesValue": "508+",
        "metricFilesLabel": "Dart Files",
        "metricScreensValue": "35+",
        "metricScreensLabel": "Screens",
        "metricServicesValue": "86+",
        "metricServicesLabel": "Services",
        "metricModelsValue": "126+",
        "metricModelsLabel": "Models"
      },
      "architecture": {
        "title": "Clean Architecture + Riverpod",
        "presentationTitle": "Presentation",
        "presentationDesc": "278+ files, Material Design 3, responsive design, ConsumerWidget/ConsumerStatefulWidget",
        "stateTitle": "State Management",
        "stateDesc": "Riverpod with AsyncNotifier/StateNotifier, stream subscriptions, real-time updates",
        "businessTitle": "Business Logic",
        "businessDesc": "Singleton services + Repository pattern, 113 files with clear responsibilities",
        "dataTitle": "Data Layer",
        "dataDesc": "Dual persistence: Hive CE (local-first) + Firebase Firestore (cloud sync)"
      },
      "features": {
        "title": "App Features",
        "gratitudeTitle": "üôè Gratitude Journal",
        "gratitudeDesc": "Gratitude diary with daily entries, categories, tags, and full-text search. Image support and social sharing.",
        "meditationTitle": "üßò Guided Meditation",
        "meditationDesc": "Audio player with meditation sessions (morning, day, anxiety, night, deep). Just Audio + Audio Session for background playback.",
        "circlesTitle": "‚≠ï Circles (Tribes)",
        "circlesDesc": "Social communities with group rituals, posts, collaborative challenges, gamification, and real-time sync via Firestore.",
        "challengesTitle": "üèÜ Challenges & Progression",
        "challengesDesc": "Daily challenges, streaks, level system, progression tree, achievements, and surprise rewards.",
        "nightRitualTitle": "üåô Night Ritual",
        "nightRitualDesc": "Guided night ritual with meditation, reflection, intentions for the next day. Scheduling with local notifications.",
        "adversityTitle": "üíä Adversity Support",
        "adversityDesc": "Mental health module with adversity journey, therapeutic practices, inspiring stories, emergency resources.",
        "timeCapsuleTitle": "‚è≥ Time Capsule",
        "timeCapsuleDesc": "Time capsules to store memories and gratitude to be opened in the future.",
        "valuesTitle": "üéØ Values & Goals",
        "valuesDesc": "Personal values definition, goal visualization, periodic reviews.",
        "analyticsTitle": "üìä Wellbeing Analytics",
        "analyticsDesc": "Wellness assessments, mood graphs (fl_chart), activity calendar (table_calendar)."
      },
      "ai": {
        "title": "AI Integration",
        "description": "Multi-backend AI engine with Google Gemini, Hugging Face, and on-device models (TensorFlow Lite).",
        "engineTitle": "AI Engine",
        "capabilitiesTitle": "AI Capabilities",
        "sentimentTitle": "Sentiment Analysis",
        "sentimentDesc": "Detects emotions in gratitude entries with caching for performance",
        "themesTitle": "Theme Detection",
        "themesDesc": "Automatically categorizes entries (family, work, health, etc.)",
        "responsesTitle": "Personalized Responses",
        "responsesDesc": "Generates reflections and suggestions based on user history",
        "premiumTitle": "AI Premium Service",
        "premiumDesc": "Advanced features for premium users with Gemini Pro",
        "aiAnalyticsTitle": "AI Analytics",
        "aiAnalyticsDesc": "Insights and trends derived from gratitude practices"
      },
      "monetization": {
        "title": "Ethical Monetization System",
        "description": "Transforming ads into \"gratitude moments\" ‚Äî monetization integrated into the app's concept, creating meaningful experience instead of intrusive.",
        "appealsTitle": "üôè Contextual Appeals",
        "appealsDesc": "Personalized gratitude messages based on user context.",
        "appealsExample1": "Post-Ritual: \"If this app helped you...\"",
        "appealsExample2": "Streak: \"7 days of gratitude! Give back...\"",
        "appealsExample3": "High Energy: \"Feel that positive vibe?\"",
        "appealsExample4": "Achievements: \"Achievement unlocked!\"",
        "tokensTitle": "üéÆ Token Economy",
        "tokensDesc": "Gamified system with tokens that can be earned and used in the app.",
        "tokensExample1": "Daily bonus with multipliers",
        "tokensExample2": "Streak multipliers",
        "tokensExample3": "Achievement rewards",
        "tokensExample4": "Challenge completion",
        "remoteConfigTitle": "üìä Firebase Remote Config",
        "remoteConfigDesc": "Dynamic control of all monetization without deploy.",
        "remoteConfigExample1": "Ad frequency",
        "remoteConfigExample2": "Cooldown periods",
        "remoteConfigExample3": "A/B testing messages",
        "remoteConfigExample4": "Conversion metrics",
        "triggersTitle": "Gratitude Triggers Service"
      },
      "firebase": {
        "title": "Firebase Full Stack",
        "authTitle": "üîê Authentication",
        "storageTitle": "üì¶ Storage",
        "analyticsTitle": "üìä Analytics",
        "messagingTitle": "üîî Messaging"
      },
      "persistence": {
        "title": "Dual Persistence: Local-First",
        "description": "Local-first architecture with Hive CE for performance and offline, automatically syncing with Firebase Firestore.",
        "hiveTitle": "Hive CE (Local)",
        "hiveDesc": "197 TypeIDs registered, code generated via build_runner, centralized adapter registry.",
        "syncTitle": "Sync Strategy",
        "syncDesc": "Last-write-wins with conflict detection, retry queue for offline, recovery service for reconnection."
      },
      "navigation": {
        "title": "Navigation with GoRouter",
        "description": "100+ declarative routes with GoRouter, deep linking, redirect guards, and typed parameters."
      },
      "i18n": {
        "title": "Internationalization",
        "portuguese": "Portuguese",
        "portugueseDesc": "Base language (app_pt.arb)",
        "english": "English",
        "englishDesc": "app_en.arb",
        "spanish": "Spanish",
        "spanishDesc": "app_es.arb",
        "summary": "200+ strings translated using ARB format (Application Resource Bundle), automatic generation via flutter gen-l10n, Python script for batch translation."
      },
      "platforms": {
        "title": "6 Supported Platforms",
        "android": "Android",
        "ios": "iOS",
        "web": "Web",
        "macos": "macOS",
        "windows": "Windows",
        "linux": "Linux"
      },
      "testing": {
        "title": "Testing & Quality",
        "unitTitle": "Unit Tests",
        "widgetTitle": "Widget Tests",
        "integrationTitle": "Integration Tests",
        "qualityTitle": "Code Quality"
      },
      "stackSection": {
        "title": "Complete Stack"
      },
      "cta": {
        "title": "Want to see more projects?",
        "description": "This is one of my most complete personal projects in Flutter. Also explore iContei (social network for counters) and the Optimus projects."
      }
    },
    "llmPool": {
      "back": "‚Üê Back",
      "badge": "Cost Optimization",
      "title": "LLM Pool Management",
      "description": "LLM pool management system with separation of tool-calling vs chat, automatic key rotation, intelligent fallback and 40% cost reduction with optimized latency.",
      "metrics": {
        "costReduction": "Cost Reduction",
        "latencyP95": "p95 Latency",
        "availability": "Availability",
        "simultaneousProviders": "Simultaneous Providers"
      },
      "problem": {
        "title": "The Problem",
        "intro": "Enterprise conversational AI systems face critical cost and latency challenges:",
        "explosiveCosts": "Explosive costs",
        "explosiveCostsDesc": "GPT-4 for everything is expensive. Not every task needs the most powerful model",
        "toolCallingExpensive": "Tool-calling is expensive",
        "toolCallingExpensiveDesc": "Models that support function calling cost more than simple models",
        "rateLimits": "Rate limits",
        "rateLimitsDesc": "A single API key hits limits quickly in production",
        "vendorLockIn": "Vendor lock-in",
        "vendorLockInDesc": "Dependency on a single provider (OpenAI) is risky",
        "keyManagement": "Key management",
        "keyManagementDesc": "API keys in code or env vars is insecure"
      },
      "solution": {
        "title": "The Solution",
        "poolArchitecture": {
          "title": "Separate Pool Architecture",
          "intro": "Pool system with explicit separation between",
          "chat": "chat",
          "chatDesc": "(conversation)",
          "and": "and",
          "tools": "tools",
          "toolsDesc": "(function calling)",
          "chatPool": "Chat Pool",
          "chatPoolOptimized": "Model optimized for conversation",
          "chatPoolLowerCost": "Lower cost (gpt-4o-mini)",
          "chatPoolHumanized": "Used for humanized responses",
          "toolsPool": "Tools Pool",
          "toolsPoolFunctionCalling": "Model with function calling",
          "toolsPoolHigherCost": "Higher cost (gpt-4o)",
          "toolsPoolOnlyWhenNeeded": "Used only when tools need to be called",
          "resultLabel": "Result",
          "resultText": "40% cost reduction by using a cheaper model for chat and a premium model only for operations that truly need tool calling."
        },
        "keyGroups": {
          "title": "Key Groups ‚Äî Key Vault",
          "intro": "Separation between",
          "pools": "pools",
          "poolsDesc": "(model configuration)",
          "and": "and",
          "keyGroupsLabel": "key groups",
          "keyGroupsDesc": "(key vault)",
          "fernetEncryption": "Fernet Encryption",
          "fernetEncryptionDesc": "Keys stored encrypted at rest",
          "fingerprint": "Fingerprint",
          "fingerprintDesc": "UI shows only fingerprint (sk-...7f2a), never the full key",
          "autoValidation": "Automatic validation",
          "autoValidationDesc": "System validates keys periodically and marks status",
          "description": "Agrupamento de API keys com rota√ß√£o autom√°tica e rate limiting distribu√≠do."
        },
        "automaticSelection": {
          "title": "Automatic Key Selection",
          "description": "The system automatically rotates between valid keys from the same key group:",
          "step1": "Request arrives for pool",
          "step2": "System fetches key_group",
          "step3": "Filters only keys with",
          "step4": "Selects key (round-robin or random)",
          "step5": "If rate limit ‚Üí marks cooldown ‚Üí tries next key"
        },
        "globalDefaults": {
          "title": "Global Defaults + Tenant Override",
          "description": "Configuration hierarchy with intelligent fallback:"
        },
        "crossPoolFallback": {
          "title": "Cross-Pool Fallback",
          "description": "When all keys from the primary pool are in cooldown:",
          "exhausted": "Chat pool exhausted (all keys in rate limit)",
          "tryTools": "Tries tools_pool (if it supports chat)",
          "tryGlobal": "Tries global defaults",
          "success": "Request completes with minimal additional latency",
          "quotaEnforcerLabel": "QuotaEnforcer",
          "quotaEnforcerText": "Tracked provider health with 60s cooldown per key."
        },
        "frontendIntegration": {
          "title": "Frontend Management",
          "description": "Complete Admin UI for pool and key management:",
          "poolManagement": "Pool Management",
          "poolCrud": "Pool CRUD",
          "toggleTools": "Toggle <code>supports_tools</code>",
          "toggleActive": "Toggle <code>is_active</code>",
          "keyGroupSelection": "Key group selection",
          "keyManagement": "Key Management",
          "addRemoveKeys": "Add/remove keys",
          "revealVault": "Reveal with vault password",
          "batchValidation": "Batch validation",
          "statusPerKey": "Status per key (valid/invalid)"
        }
      },
      "architecture": {
        "title": "Architecture",
        "description": "Vis√£o geral do sistema de pools."
      },
      "technicalDecisions": {
        "title": "Technical Decisions",
        "whySeparate": "Why separate pools and key groups?",
        "whySeparateAnswer": "Pools define model configuration (model, temperature, supports_tools). Key groups define credentials (provider_type, base_url, keys). This separation allows reusing the same key vault across multiple pools with different models.",
        "whyFailFast": "Why fail-fast and not fallback to env vars?",
        "whyFailFastAnswer": "In production, keys in code/env vars are a security anti-pattern. Fail-fast forces correct configuration in the system and avoids \"works on my machine\" with different keys.",
        "whyCache60s": "Why 60s cache for provider configs?",
        "whyCache60sAnswer": "Balance between consistency and performance. 60s is enough to propagate configuration changes without bombarding Memory Engine on every request. TTLCache with thread-safe lock.",
        "whyCooldown60s": "Why QuotaEnforcer with 60s cooldown?",
        "whyCooldown60sAnswer": "Provider rate limits (OpenAI, Groq) typically reset in 60s. Cooldown avoids retry storms that would worsen the rate limit. Cross-pool fallback ensures continuity."
      },
      "techStack": "Tech Stack",
      "nextProject": "Next: AI Conversation Engine ‚Üí",
      "decisions": {
        "title": "Decis√µes T√©cnicas",
        "separatePools": {
          "question": "Por que pools separados para chat vs tools?",
          "answer": "Modelos de tool-calling (GPT-4, Claude) custam 10-20x mais que modelos simples. Separar permite usar GPT-3.5 para conversa√ß√£o e reservar GPT-4 s√≥ para quando precisa chamar ferramentas."
        },
        "keyRotation": {
          "question": "Por que rota√ß√£o de keys?",
          "answer": "Rate limits s√£o por key. Com m√∫ltiplas keys rotacionando, o throughput efetivo multiplica. Tamb√©m permite revogar keys comprometidas sem downtime."
        },
        "vault": {
          "question": "Por que vault criptografado?",
          "answer": "API keys s√£o segredos cr√≠ticos. Armazenar em env vars ou c√≥digo √© vulner√°vel. Vault com encryption at-rest e audit logging √© enterprise-grade."
        },
        "fallback": {
          "question": "Por que fallback inteligente?",
          "answer": "Providers t√™m outages. Fallback autom√°tico para outro provider garante disponibilidade mesmo quando OpenAI est√° fora."
        }
      },
      "cta": {
        "title": "Explore Outros Case Studies",
        "description": "Veja como outros componentes do Optimus foram constru√≠dos"
      },
      "footer": "Case Study: LLM Pool Management ‚Äî Optimus AI Platform",
      "meta": {
        "title": "LLM Pool Management - Optimus AI | Marcelo Marleta",
        "description": "Case study: Sistema de gest√£o de pools LLM com separa√ß√£o tool-calling vs chat, rota√ß√£o de keys e 40% de redu√ß√£o de custos."
      }
    },
    "pricing": {
      "back": "‚Üê Back",
      "badge": "RAG + ML",
      "title": "Pricing Intelligence",
      "description": "Price query system with optimized RAG, adaptive threshold, request coalescing and multi-tier cache for sub-second responses.",
      "metrics": {
        "precision": "Precision",
        "cacheHit": "Cache Hit",
        "cacheRate": "Cache Rate",
        "embeddingDim": "Embedding Dim"
      },
      "problem": {
        "title": "The Problem",
        "intro": "Price queries in clinics are complex: patients ask using different names (\"x-ray\", \"radiography\", \"dental rx\"), with typos, and expect instant responses. Traditional RAG fails at:",
        "fuzzyMatching": "Fuzzy matching of procedure names",
        "multipleItems": "Multiple items in the same question (\"how much does cleaning and whitening cost?\")",
        "acceptableLatency": "Acceptable latency under load (thundering herd)"
      },
      "solution": {
        "title": "The Solution",
        "pricingFastLane": {
          "title": "Pricing Fast Lane",
          "description": "Optimized bypass that detects pricing intent and executes direct search, without going through the full agent loop:",
          "fastLaneComment": "# Fast Lane Flow",
          "normalFlowComment": "# vs Normal Flow"
        },
        "adaptiveThreshold": {
          "title": "Adaptive Threshold v2",
          "description": "Dynamic similarity threshold based on query characteristics:",
          "shortQuery": "Short query",
          "shortQueryDesc": "(<3 words): lower threshold (0.75)",
          "numbersQuery": "Query with numbers",
          "numbersQueryDesc": "threshold adjusted for abbreviations",
          "multiItemQuery": "Multi-item query",
          "multiItemQueryDesc": "relaxed threshold + diversification",
          "llmOverride": "LLM override",
          "llmOverrideDesc": "model can suggest specific threshold",
          "mediumQuery": "Query m√©dia",
          "longQuery": "Query longa"
        },
        "requestCoalescing": {
          "title": "Request Coalescing",
          "description": "Thundering herd prevention via distributed lock:",
          "intro": "When multiple requests arrive for the same query:",
          "step1": "First request acquires lock and computes result",
          "step2": "Subsequent requests wait (with timeout)",
          "step3": "Result is shared among all",
          "step4": "Cache is populated for next requests"
        }
      },
      "cacheArchitecture": {
        "title": "Cache Architecture",
        "description": "Cache multi-tier com TTLs diferenciados por tipo de dado."
      },
      "intelligentCacheKey": {
        "title": "Intelligent Cache Key",
        "intro": "Instead of raw query hash (which fails with variations), we use",
        "semanticSignature": "semantic signature",
        "basedOnItems": "based on detected items:",
        "traditional": "Traditional Cache Key",
        "semantic": "Semantic Cache Key",
        "description": "Normaliza√ß√£o de queries para maximizar cache hits."
      },
      "techStack": "Tech Stack",
      "nextProject": "Next: Document Processing ‚Üí",
      "cta": {
        "title": "Explore Outros Case Studies",
        "description": "Veja como outros componentes do Optimus foram constru√≠dos"
      },
      "footer": "Case Study: Pricing Intelligence ‚Äî Optimus AI Platform",
      "meta": {
        "title": "Pricing Intelligence - Optimus AI | Marcelo Marleta",
        "description": "Case study: Sistema de consulta de pre√ßos com RAG otimizado, threshold adaptativo e cache multi-tier."
      }
    },
    "frontend": {
      "backToPortfolio": "Back to Portfolio",
      "badge": "User Interface",
      "title": "Frontend Apps",
      "description": "Two specialized Vue.js 3 applications: Admin Dashboard for each tenant's operators and SuperAdmin Panel for global platform management. Real-time WebSocket, handover management, and responsive interfaces.",
      "architectureTitle": "Architecture: 2 Apps, 2 Purposes",
      "rbac": {
        "title": "RBAC (Role-Based Access Control)",
        "admin": "admin ‚Üí Access to the tenant Admin App (3000)",
        "superadmin": "superadmin ‚Üí Access to SuperAdmin (3001) + all tenants"
      },
      "adminDashboard": {
        "title": "Admin Dashboard",
        "port": "Port 3000",
        "description": "Interface for operators and admins of each tenant. Focus on customer support and day-to-day management.",
        "featuresTitle": "Features:",
        "features": {
          "chat": "Real-time chat with customers (WebSocket)",
          "handover": "Handover management (30min auto-return timer)",
          "dashboard": "Tenant metrics dashboard",
          "conversations": "List of active conversations",
          "history": "Support history",
          "notifications": "New message notifications"
        }
      },
      "superAdminPanel": {
        "title": "SuperAdmin Panel",
        "port": "Port 3001",
        "description": "Interface for global platform management. Configuration, tenant provisioning, and monitoring.",
        "featuresTitle": "Features:",
        "features": {
          "tenants": "Tenant management (CRUD)",
          "users": "User and permissions management",
          "config": "Global platform configuration",
          "monitoring": "Cross-tenant monitoring",
          "analytics": "Analytics and reports",
          "provisioning": "Provisioning of new tenants",
          "prompts": "Per-tenant prompt configuration",
          "llmPools": "LLM Pools & Key Groups management",
          "preflight": "Preflight panel (health checks)",
          "defaults": "System defaults (global configs)"
        }
      },
      "websocket": {
        "title": "Real-time WebSocket",
        "description": "Bidirectional communication for instant updates.",
        "events": {
          "newMessage": "new_message: New customer message",
          "statusChange": "status_change: Conversation status change",
          "handoverUpdate": "handover_update: Handover update"
        }
      },
      "handover": {
        "title": "Handover Management",
        "description": "Complete AI ‚Üî Human transfer system.",
        "features": {
          "takeover": "Manual takeover by operator",
          "autoReturn": "Auto-return after 30min of inactivity",
          "history": "Transfer history"
        }
      },
      "components": {
        "title": "Reusable Components",
        "description": "Shared Vue.js component library between the two applications."
      },
      "techStack": {
        "title": "Technical Stack",
        "framework": "Framework",
        "styling": "Styling",
        "state": "State",
        "realtime": "Real-time",
        "router": "Router",
        "http": "HTTP",
        "build": "Build",
        "container": "Container",
        "backendIntegration": {
          "title": "Backend Integration",
          "restApis": "REST APIs",
          "websocket": "WebSocket"
        }
      },
      "cta": {
        "title": "Explore Other Case Studies",
        "description": "See how other Optimus components were built",
        "button": "Contact"
      },
      "footer": "Case Study: Frontend Apps ‚Äî Optimus AI Platform",
      "meta": {
        "title": "Frontend Apps - Optimus AI | Marcelo Marleta",
        "description": "Case study: Vue.js 3 applications for support management with real-time WebSocket and RBAC."
      },
      "chatRealtime": {
        "title": "Real-time Chat with WebSocket",
        "description": "The chat interface uses WebSocket for real-time updates. When a new message arrives (WhatsApp, web, etc.), the operator sees it instantly without refresh.",
        "flowTitle": "WebSocket Flow",
        "eventsTitle": "Supported Events",
        "events": {
          "newMessage": "New message in conversation",
          "conversationUpdated": "Status or metadata changed",
          "handoverStarted": "Operator took over conversation",
          "handoverEnded": "Conversation returned to AI",
          "typing": "Customer is typing"
        }
      },
      "handoverManagement": {
        "title": "Handover Management",
        "description": "When an operator takes over a conversation (handover), the UI shows a 30-minute timer. If there's no interaction, the conversation automatically returns to the AI.",
        "states": {
          "aiServing": {
            "title": "1. AI Serving",
            "description": "Normal state. AI replies automatically. Operator can view but doesn't intervene."
          },
          "handoverActive": {
            "title": "2. Handover Active",
            "description": "Operator took over. 30 min timer visible. AI pauses, messages go directly to the operator."
          },
          "timerExpires": {
            "title": "3. Timer Expires",
            "description": "No action in 30min ‚Üí auto-return to AI. Operator is notified beforehand (warning at 5min)."
          }
        },
        "timerUiTitle": "Timer UI Component",
        "extend": "Extend +15min",
        "returningSoon": "Returning soon!",
        "timeRemaining": "Time remaining"
      },
      "superAdminFeatures": {
        "title": "SuperAdmin: Global Management",
        "tenantManagement": {
          "title": "Tenant Management",
          "list": "Tenant list with status, vertical, and last activity",
          "create": "Create tenant via wizard (choose vertical, configure identity)",
          "edit": "Edit tenant (name, timezone, locale, features)",
          "deactivate": "Deactivate/Delete with confirmation and soft-delete"
        },
        "promptConfig": {
          "title": "Prompt Configuration",
          "systemPrompt": "System prompt customized per tenant",
          "greeting": "Greeting message (AI's first message)",
          "intent": "Intent classification prompt",
          "handoverContext": "Handover context (summary for operator)",
          "variables": "Variables override (assistant_name, rules, etc.)"
        },
        "llmPools": {
          "title": "LLM Pools & Keys",
          "globalPools": "Global pools (chat, tools, embeddings)",
          "keyGroups": "Key Groups with multiple API keys",
          "reveal": "Protected reveal (only superadmin sees keys)",
          "health": "Health status for each key/pool"
        },
        "preflightPanel": {
          "title": "Preflight Panel",
          "ok": "OK - All configs valid",
          "warning": "Warning - Using generic defaults",
          "critical": "Critical - Missing assistant_name or empty template",
          "sourceBadges": "Source badges - Seeded vs Override vs Default"
        }
      },
      "dashboard": {
        "title": "Multi-tenant Dashboard",
        "description": "The Admin Dashboard shows metrics specific to the logged-in tenant. Active conversations, average response time, handovers, and more.",
        "metrics": {
          "conversationsToday": "Conversations Today",
          "activeNow": "Active Now",
          "inHandover": "In Handover",
          "responseTime": "Response Time"
        },
        "conversationsList": {
          "title": "Conversations List",
          "lastMsg": "Last msg",
          "aiResponding": "AI Responding",
          "handover": "Handover",
          "operator": "Operator"
        }
      },
      "results": {
        "title": "Results",
        "websocketLatency": "WebSocket Latency",
        "websocketLatencyDesc": "Real-time messages",
        "specializedApps": "Specialized Apps",
        "specializedAppsDesc": "Admin + SuperAdmin",
        "accessControl": "Access Control",
        "accessControlDesc": "admin vs superadmin",
        "autoReturnHandover": "Auto-return Handover",
        "autoReturnHandoverDesc": "Configurable timer"
      }
    },
    "infraDevops": {
      "back": "Back",
      "platformName": "Optimus AI Platform",
      "badge": "Infrastructure & DevOps",
      "badgeSecondary": "Platform Engineering",
      "title": "Infrastructure &",
      "titleHighlight": "DevOps Pipeline",
      "description": "Production-grade containerized architecture with automatic horizontal scaling, high availability via Redis Sentinel, and CI/CD with integrated architecture validation. From prototyping to scaling multiple instances without downtime.",
      "metrics": {
        "microservices": "Microservices",
        "composeOverlays": "Compose Overlays",
        "sentinelsHa": "Sentinels HA",
        "workflowsCicd": "CI/CD Workflows"
      },
      "nav": {
        "problem": "Problem",
        "containers": "Containers",
        "scaling": "Scaling",
        "ha": "High Availability",
        "cicd": "CI/CD",
        "security": "Security",
        "operations": "Operations",
        "results": "Results"
      },
      "problem": {
        "title": "The Problem",
        "description": "Multi-tenant AI platforms have unique characteristics that complicate traditional infrastructure: highly variable workloads (a prompt can take 100ms or 30s), critical dependency on external services (LLMs, WhatsApp), and the need to isolate tenants while efficiently sharing resources.",
        "challengesTitle": "Specific Challenges",
        "challenges": {
          "spikyTraffic": "Marketing campaigns can 10x traffic in minutes ‚Äî needs to scale fast and scale back without wasting resources",
          "statefulServices": "Memory Engine and pgvector need controlled connection pools ‚Äî it's not just \"spawn more containers\" and done",
          "redisSPOF": "Cache, sessions, queues, rate limiting ‚Äî everything goes through Redis. Needs real HA",
          "microservices": "Orchestrate coordinated deploys without breaking changes between differently versioned services",
          "secretsSprawl": "40+ API keys from LLMs, webhooks, databases ‚Äî need rotation without downtime"
        },
        "solution": "The solution was built iteratively: started with a monolithic docker-compose, evolved to environment overlays, then horizontal scaling with nginx LB, and finally Redis Sentinel for HA. Each step was motivated by real production needs."
      },
      "containers": {
        "title": "Container Architecture",
        "description": "Dockerfiles follow production-grade patterns: multi-stage builds for smaller images, non-root users for security, BuildKit cache mounts for fast builds, and health checks that actually test the application (not just if the process exists).",
        "dockerfileTitle": "Production Patterns",
        "overlaysTitle": "Docker Compose Overlays",
        "overlaysDescription": "Instead of a monolithic docker-compose.yml, we use overlays that compose configurations. This allows development, staging, and production to share the base but customize what they need ‚Äî no duplication and no accidental drift.",
        "overlays": {
          "base": "Base: all services, volumes, networks. Common configuration that works in any environment.",
          "scale": "Overlay: nginx-lb, separate migrations job, ports removed (all via LB), configurable WEB_CONCURRENCY.",
          "sentinel": "Overlay: Redis master/replica/sentinel topology for HA. Services receive REDIS_SENTINEL_* vars.",
          "dev": "Overlay: mounted code volumes, debug enabled, hot reload, ports exposed directly."
        },
        "compositionTitle": "Overlay Composition",
        "resourceLimitsTitle": "Resource Limits & Reservations"
      },
      "scaling": {
        "title": "Horizontal Scaling",
        "description": "Horizontal scaling in Docker Compose seems simple (--scale ai-engine=3), but has pitfalls. Ports conflict, migrations run multiple times, and load balancing needs dynamic DNS resolution. The solution was a dedicated nginx-lb with specific configuration for scaled containers.",
        "architectureTitle": "Scaling Architecture",
        "externalTraffic": "External Traffic",
        "dynamicDns": "least_conn + dynamic DNS",
        "nginxLbTitle": "Nginx Load Balancer",
        "migrationJobTitle": "Isolated Migration Job",
        "migrationJobDescription": "When you scale services, they all try to run migrations on startup ‚Äî guaranteed race condition. The solution was a dedicated job that runs BEFORE replicas come up, using depends_on: condition: service_completed_successfully.",
        "scaleUpScriptTitle": "Scale-Up Script",
        "statefulConsiderations": {
          "title": "Stateful Considerations",
          "description": "Memory Engine is stateful ‚Äî maintains connection pools with PostgreSQL. Scaling from 2 to 6 replicas creates 6x more database connections. That's why we configure DB_POOL_SIZE and DB_MAX_OVERFLOW via env vars, allowing dynamic adjustment:"
        },
        "nginxTitle": "Nginx Load Balancer",
        "scriptTitle": "Script de Scaling",
        "sessionAffinityTitle": "Session Affinity"
      },
      "ha": {
        "title": "High Availability",
        "description": "Redis is too critical to be a single point of failure: cache, sessions, rate limiting, Celery queues, WebSocket state. The solution was Redis Sentinel ‚Äî automatic replication with failover managed by consensus of 3 sentinels.",
        "topologyTitle": "Redis Sentinel Topology",
        "quorum": "quorum: 2 (2 of 3 need to agree)",
        "monitors": "monitors",
        "writes": "(writes)",
        "reads": "(reads)",
        "failoverTitle": "Automatic Failover",
        "failoverSteps": {
          "step1": "Master becomes inaccessible (crash, network partition, or any failure)",
          "step2": "Sentinels detect (configurable timeout, default 30s)",
          "step3": "Quorum of 2 sentinels agrees that master is down",
          "step4": "Election of new master among replicas (based on replication offset)",
          "step5": "Sentinels update configuration; clients reconnect automatically"
        },
        "whyThreeSentinels": {
          "title": "Why 3 Sentinels?",
          "description": "With quorum of 2, you need at least 3 sentinels to tolerate 1 failure. If you have only 2 sentinels and 1 fails, quorum isn't reached and failover doesn't happen. Odd number avoids split-brain (tie votes)."
        },
        "clientConfigTitle": "Configura√ß√£o do Cliente"
      },
      "cicd": {
        "title": "CI/CD Pipeline",
        "description": "The pipeline isn't just lint + test + deploy. It includes architecture validation that blocks PRs that violate project guardrails, LLM evaluation for AI Engine changes, and automatic sync of OpenAPI specs.",
        "workflows": {
          "ci": {
            "title": "ci.yml",
            "description": "Ruff lint, mypy type-check, pytest, Docker build. Runs on every push/PR."
          },
          "guardrails": {
            "title": "guardrails.yml",
            "description": "Architecture validation. Blocks PRs that violate patterns defined in policy.yaml."
          },
          "e2e": {
            "title": "e2e-tests.yml",
            "description": "End-to-end tests with real pgvector + Redis. Validates multi-ERP flows."
          },
          "llmEval": {
            "title": "llm-eval.yml",
            "description": "AI Engine output quality evaluation. Detects prompt regressions."
          },
          "lint": "Lint & Format: black, isort, flake8, mypy",
          "test": "Testes: pytest com coverage m√≠nimo de 80%",
          "build": "Build: Docker multi-stage com cache",
          "deploy": "Deploy: Rolling update com health checks"
        },
        "guardrailsTitle": "Architecture Guardrails",
        "guardrailsDescription": "The guardrails system uses an MCP server that analyzes diffs and validates against rules. If a PR violates a pattern (e.g., direct import between layers, raw SQL in controllers), the workflow fails and posts a detailed comment on the PR.",
        "e2eTestsTitle": "E2E Tests with Services",
        "workflowsTitle": "Workflows Principais",
        "archGuardsTitle": "Architecture Guards"
      },
      "security": {
        "title": "Security",
        "description": "With 40+ API keys (LLMs, WhatsApp, databases, observability), secrets management can't be improvised. The secrets-manager.sh script centralizes creation, validation, encrypted backup, and secret rotation.",
        "secretsSeparationTitle": "Secrets Separation",
        "secretsFiles": {
          "secrets": "API keys, passwords, tokens. Never committed. Gitignore'd.",
          "models": "LLM model configurations. Can be committed (no keys).",
          "example": "Template with placeholders. Committed for documentation."
        },
        "containerSecurityTitle": "Container Security",
        "containerSecurity": {
          "nonRoot": "All containers run as non-root user (appuser:1001 or similar). Mitigates privilege escalation.",
          "slimImages": "python:3.12-slim instead of full. Fewer packages = smaller attack surface.",
          "readOnlyConfigs": "Config files mounted as :ro (read-only). Containers can't modify them.",
          "networkIsolation": "Dedicated bridge network (optimus). Services only accessible via LB.",
          "internalEngine": "Port 8050 not exposed to host ‚Äî only accessible internally via nginx-lb."
        },
        "secretsTitle": "Secrets Management",
        "networkTitle": "Network Isolation",
        "scanningTitle": "Scanning Automatizado"
      },
      "operations": {
        "title": "Operational Scripts",
        "description": "Day-to-day operations are automated in well-documented scripts. This reduces human error and allows any dev to perform production operations safely.",
        "scripts": {
          "scaleUp": {
            "title": "scale-up.sh",
            "description": "Complete scaling with build, migrations, and log tail."
          },
          "validateRedis": {
            "title": "validate_redis.sh",
            "description": "Redis health check: ping, memory, connected clients."
          },
          "loadTests": {
            "title": "run_load_tests.sh",
            "description": "Load testing with k6 or locust. Generates report."
          },
          "traceSmokeTest": {
            "title": "trace_smoke_test.sh",
            "description": "Validates E2E tracing: generates trace, verifies in Tempo."
          }
        },
        "healthChecksTitle": "Useful Health Checks",
        "backupTitle": "Backup & Restore",
        "logsTitle": "Log Aggregation",
        "monitoringTitle": "Monitoring"
      },
      "results": {
        "title": "Results",
        "metrics": {
          "deployTime": "Complete deploy (build + migrations + scale)",
          "uptime": "Uptime with Redis Sentinel HA",
          "secretsExposed": "Secrets exposed (automatic validation)",
          "failover": "Automatic Redis failover",
          "scalingTime": "Scaling Time",
          "mttr": "MTTR"
        },
        "lessonsLearned": {
          "title": "Lessons Learned",
          "overlays": "Compose overlays > monolithic: Much easier maintenance when each concern is isolated",
          "migrations": "Separate migrations job: Avoids race conditions and allows scaling replicas freely",
          "healthChecks": "Real health checks: Test the /health endpoint, not just whether the process exists",
          "dynamicDns": "Dynamic DNS on the LB: Without it, nginx doesn't discover new replicas",
          "sentinelQuorum": "Odd Sentinel quorum: 3 sentinels with quorum 2 is the minimum for real HA"
        }
      },
      "techStack": "Technical Stack",
      "cta": {
        "title": "Explore Other Case Studies",
        "description": "See how other Optimus components were built"
      },
      "footer": "Case Study: Infrastructure & DevOps ‚Äî Optimus AI Platform",
      "meta": {
        "title": "Infrastructure & DevOps - Optimus AI | Marcelo Marleta",
        "description": "Case study: Production-grade containerized architecture with Docker Compose, horizontal scaling via Nginx LB, Redis Sentinel for HA, and CI/CD with architecture validation."
      }
    },
    "testTools": {
      "back": "Back",
      "platformName": "Optimus AI Platform",
      "badge": "Quality Assurance",
      "badgeSecondary": "AI Testing",
      "title": "AI Testing",
      "titleHighlight": "Tools",
      "description": "Complete testing suite for conversational AI: multi-channel simulation interface, hallucination detector, multi-dimensional quality scorer, and realistic scenarios with configurable customer personalities.",
      "metrics": {
        "analyzers": "Analyzers",
        "personalities": "Personalities",
        "realtime": "Real-time",
        "multichannel": "Multi",
        "websocket": "WebSocket",
        "channel": "Channel"
      },
      "nav": {
        "problem": "Problem",
        "interface": "Interface",
        "hallucination": "Hallucination Detector",
        "quality": "Quality Scorer",
        "scenarios": "Scenarios",
        "runner": "Test Runner"
      },
      "problem": {
        "title": "The Problem",
        "description": "Testing conversational AI systems is fundamentally different from testing traditional software. Outputs are probabilistic, context matters, and \"correct\" is subjective.",
        "challengesTitle": "Specific AI Testing Challenges",
        "challenges": {
          "hallucinations": "AI invents information that seems plausible but is false. In medical/dental context, this is dangerous.",
          "consistency": "Same question can generate different answers. How do you test that?",
          "multiTurn": "Response at turn 5 depends on turns 1-4. Isolated tests don't capture this.",
          "subjectivity": "\"Good response\" depends on tone, empathy, completeness ‚Äî metrics that are hard to quantify."
        },
        "solution": "The solution was to create a suite of specialized tools: simulation interface for manual testing, automatic analyzers for problem detection, and a framework of realistic scenarios for regression testing."
      },
      "interface": {
        "title": "Simulation Interface",
        "description": "Flask + SocketIO web interface for interactive manual testing. Simulates different channels (WhatsApp, Web, API) with correct headers, manages handover, and shows real-time metrics.",
        "features": {
          "channelProfiles": {
            "title": "Channel Profiles",
            "description": "WhatsApp, Web, API ‚Äî each with specific headers (X-Request-Source)."
          },
          "handover": {
            "title": "Handover Management",
            "description": "Takeover, Return to AI, Operator Messages ‚Äî tests complete flow."
          },
          "multiSession": {
            "title": "Multi-Session",
            "description": "Multiple simultaneous sessions to simulate load."
          },
          "realtime": {
            "title": "Real-time",
            "description": "WebSocket for instant response feedback."
          }
        },
        "scenariosTitle": "Built-in Test Scenarios"
      },
      "hallucination": {
        "title": "Hallucination Detector",
        "description": "Detects hallucinations and factual errors in responses. Critical for medical/dental context where false information can cause real harm.",
        "typesTitle": "Detected Hallucination Types",
        "types": {
          "factualError": {
            "title": "factual_error",
            "description": "Information contradicts known facts (e.g., \"cleaning costs R$5000\")."
          },
          "inventedInfo": {
            "title": "invented_info",
            "description": "AI invents specific details not provided (e.g., dentist's name)."
          },
          "impossibleClaim": {
            "title": "impossible_claim",
            "description": "Logically impossible statements (e.g., \"5-minute consultation\")."
          },
          "medicalMisinfo": {
            "title": "medical_misinformation",
            "description": "Incorrect or dangerous medical information."
          }
        },
        "knowledgeBaseTitle": "Knowledge Base for Validation"
      },
      "quality": {
        "title": "Quality Scorer",
        "description": "Evaluates response quality across multiple dimensions. It's not just \"right or wrong\" ‚Äî it's readability, relevance, empathy, completeness, tone.",
        "dimensionsTitle": "Quality Dimensions",
        "dimensions": {
          "readability": "Flesch-Kincaid grade level. Responses should be accessible to the general public.",
          "empathy": "Detects empathy indicators: \"I understand\", \"I see\", \"I'm sorry\".",
          "callToAction": "Does the response lead to next steps? \"schedule\", \"call\", \"visit\".",
          "professionalLanguage": "Use of professional terms: \"consultation\", \"treatment\", \"procedure\"."
        },
        "detractorsTitle": "Quality Detractors (reduce score)"
      },
      "scenarios": {
        "title": "Realistic Scenarios",
        "description": "Generation of test scenarios with different complexity levels and customer personalities. Allows systematic testing of edge cases and regressions.",
        "validationsTitle": "Scenario Validations"
      },
      "runner": {
        "title": "Test Suite Runner",
        "description": "Test orchestrator with support for parallelization, coverage, and HTML reports. Integrates with pytest and offers rich CLI via Rich.",
        "markersTitle": "Custom Pytest Markers",
        "markers": {
          "slow": {
            "title": "{'@'}pytest.mark.slow",
            "description": "Slow tests, skipped by default. Use --runslow to include."
          },
          "integration": {
            "title": "{'@'}pytest.mark.integration",
            "description": "Tests that need running services."
          },
          "aiQuality": {
            "title": "{'@'}pytest.mark.ai_quality",
            "description": "AI quality tests (hallucination, quality score)."
          },
          "security": {
            "title": "{'@'}pytest.mark.security",
            "description": "Security tests (SQL injection, XSS)."
          }
        }
      },
      "techStack": "Technical Stack",
      "results": {
        "title": "Results",
        "metrics": {
          "analyzers": "Specialized analyzers",
          "coverage": "Critical scenario coverage",
          "hallucinations": "Hallucinations in production",
          "realtime": "Feedback via WebSocket"
        }
      },
      "cta": {
        "title": "Explore Other Case Studies",
        "description": "See how other Optimus components were built"
      },
      "footer": "Case Study: AI Testing Tools ‚Äî Optimus AI Platform",
      "meta": {
        "title": "AI Testing Tools - Optimus AI | Marcelo Marleta",
        "description": "Case study: Testing suite for conversational AI with hallucination detector, quality scorer, realistic scenarios and multi-channel simulation interface."
      }
    },
    "memory": {
      "badge": "Memory System",
      "title": "Memory Engine",
      "description": "Multi-tier hierarchical memory system (Hot/Warm/Cold) with intelligent semantic compression, automatic LGPD/HIPAA compliance, and context injection for AI to Human continuity.",
      "metrics": {
        "latency": "p99 Latency",
        "tokenReduction": "Token Reduction",
        "complianceRules": "Compliance Rules",
        "durability": "Durability"
      },
      "problem": {
        "title": "The Problem",
        "intro": "Enterprise chatbots need context memory. When a customer returns days later, the bot needs to remember previous conversations, preferences, and support history.",
        "tokensCost": "Tokens are expensive",
        "tokensCostDesc": "Injecting the entire history into the prompt explodes costs",
        "latencyKillsUx": "Latency kills UX",
        "latencyKillsUxDesc": "Relational database queries add 200-500ms",
        "complianceMandatory": "Compliance is mandatory",
        "complianceMandatoryDesc": "LGPD, CFM, CFO have strict retention rules",
        "handoverComplicates": "Handover complicates",
        "handoverComplicatesDesc": "AI needs to know what happened when human was attending"
      },
      "solution": {
        "title": "The Solution",
        "hotWarmCold": {
          "title": "Hot/Warm/Cold Architecture",
          "intro": "3-tier storage system with Write-Through strategy:",
          "hot": "HOT",
          "hotDesc": "Redis | TTL 1min",
          "hotLatency": "<10ms latency",
          "warm": "WARM",
          "warmDesc": "Redis | TTL 1h",
          "warmLatency": "<50ms latency",
          "cold": "COLD",
          "coldDesc": "PostgreSQL",
          "coldLatency": "~100ms | Permanent",
          "writeThrough": "Write-Through",
          "writeThroughDesc": "Every write goes to PostgreSQL first (guaranteed durability), then replicates to Redis (performance)."
        },
        "customerFacts": {
          "title": "Temporal Customer Facts",
          "intro": "Instead of storing raw conversations, we extract structured facts with temporal validity windows:",
          "footer": "Complete temporal history for auditing. O(1) query for current facts via partial index."
        },
        "semanticCompression": {
          "title": "Semantic Compression with LLM",
          "intro": "Long conversations are compressed using OpenAI with vertical-specific templates (dental, medical, legal):",
          "preserves": "Preserves",
          "preservesDesc": "Critical facts, decisions, next steps",
          "removes": "Removes",
          "removesDesc": "Greetings, redundant confirmations, operational details",
          "result": "Result",
          "resultDesc": "90% reduction with 100% preservation of relevant information"
        },
        "compliance": {
          "title": "Automatic Compliance",
          "intro": "Automatic classification and treatment based on vertical:",
          "lgpd": "LGPD",
          "lgpdDesc": "Brazil",
          "hipaa": "HIPAA",
          "hipaaDesc": "USA - Healthcare",
          "cfo": "CFO",
          "cfoDesc": "Dentistry",
          "cfm": "CFM",
          "cfmDesc": "Medicine",
          "oab": "OAB",
          "oabDesc": "Law",
          "gdpr": "GDPR",
          "gdprDesc": "Europe",
          "footer": "Smart anonymization: CPF -> ***.***.***.00 | Email -> ***{'@'}domain.com"
        },
        "handover": {
          "title": "Handover Summary",
          "intro": "Perfect continuity when human agent intervenes:",
          "step1": "Operator messages -> Redis buffer (hot path)",
          "step2": "Closing -> Celery worker generates summary with OpenAI",
          "step3": "AI resumes -> Receives structured summary injected into context"
        }
      },
      "architecture": {
        "title": "Architecture",
        "lastMsg": "Last msg",
        "activeSession": "Active Session",
        "completeHistory": "Complete History",
        "writeThroughStrategy": "Write-Through Strategy",
        "sourceOfTruth": "(PostgreSQL = Source of Truth)",
        "contextComposer": "CONTEXT COMPOSER v2",
        "factsLogic": "Facts (Logic)",
        "summaryLanguage": "Summary (Language)",
        "recentChat": "Recent (Chat)",
        "lastMsgs": "[last 5 msgs]"
      },
      "decisions": {
        "title": "Technical Decisions",
        "writeThrough": {
          "question": "Why Write-Through and not Write-Behind?",
          "answer": "Write-Behind (async write to DB) is more performant but risky. In customer service, losing a message is unacceptable. Write-Through guarantees immediate durability with acceptable performance."
        },
        "schemaIsolation": {
          "question": "Why schema isolation and not row-level security?",
          "answer": "RLS adds overhead on every query. With separate schemas (t_{'{'}tenant_id{'}'}), isolation is physical and performance is maximum. Trade-off: more operational complexity."
        },
        "temporalFacts": {
          "question": "Why temporal Customer Facts?",
          "answer": "LLMs are terrible at forgetting. If the customer changed preferences, the model with complete history would keep using the old preference. Temporal facts with valid_at/invalid_at solve this elegantly."
        },
        "factsSummary": {
          "question": "Why separate Facts vs Summary in Context Composer?",
          "answer": "Facts are for logic (the system uses them for decisions). Summary is for language (the LLM uses it for natural responses). This separation prevents the LLM from hallucinating about structured data."
        }
      },
      "nextProject": "Next: AI Conversation Engine ->"
    },
    "backend": {
      "badge": "Intelligent Gateway",
      "title": "Backend Orchestrator",
      "description": "Enterprise multi-tenant gateway with distributed rate limiting, circuit breaker, dual-mode handover (operator + AI-initiated), real-time WebSocket and fail-closed resilience.",
      "metrics": {
        "gatewayLatency": "Gateway Latency",
        "reqPerMinTenant": "Req/min/tenant",
        "retryStrategies": "Retry Strategies",
        "uptime": "Uptime"
      },
      "problem": {
        "title": "The Problem",
        "intro": "In multi-tenant service systems, the gateway is the critical point of failure. Common problems:",
        "noisyNeighbor": "Noisy Neighbor",
        "noisyNeighborDesc": "One tenant with high traffic takes down all others",
        "cascadingFailures": "Cascading Failures",
        "cascadingFailuresDesc": "Slow AI Engine blocks the entire system",
        "handoverChaos": "Handover Chaos",
        "handoverChaosDesc": "AI responds when human is attending",
        "idempotencyBugs": "Idempotency Bugs",
        "idempotencyBugsDesc": "Same message processed multiple times",
        "frontendDesync": "Frontend Desync",
        "frontendDesyncDesc": "UI does not reflect real conversation state"
      },
      "solution": {
        "title": "The Solution",
        "rateLimiting": {
          "title": "Enterprise Rate Limiting",
          "intro": "Multi-layer rate limiting system with adaptive algorithms:",
          "slidingWindow": "Sliding Window",
          "slidingWindowDesc": "Temporal precision with Redis ZSET",
          "tokenBucket": "Token Bucket",
          "tokenBucketDesc": "Burst handling with atomic Lua scripts",
          "localFallback": "Local Fallback",
          "localFallbackDesc": "Local cache when Redis unavailable",
          "tenantTiers": "Tenant Tiers",
          "tenantTiersDesc": "FREE (10 req/min) -> BASIC (50) -> PROFESSIONAL (200) -> ENTERPRISE (unlimited)"
        },
        "circuitBreaker": {
          "title": "Circuit Breaker Pattern",
          "intro": "Protection against cascading failures with 3 states:",
          "closed": "CLOSED",
          "open": "OPEN",
          "halfOpen": "HALF_OPEN",
          "failureThreshold": "failure_threshold",
          "failureThresholdDesc": "5 consecutive failures open the circuit",
          "recoveryTimeout": "recovery_timeout",
          "recoveryTimeoutDesc": "60s before testing again",
          "halfOpenMaxCalls": "half_open_max_calls",
          "halfOpenMaxCallsDesc": "10 test requests before closing"
        },
        "handover": {
          "title": "Handover Dual-Mode",
          "intro": "Two modes of AI to Human transition:",
          "operatorTakeover": "Operator Takeover",
          "operatorTakeoverList": [
            "Operator clicks take over conversation",
            "Redis latch blocks AI instantly",
            "Auto-resume timer configurable per tenant",
            "Lazy finalize: summary generated on resume"
          ],
          "aiInitiated": "AI-Initiated",
          "aiInitiatedList": [
            "AI detects problem (ERP failure, complexity)",
            "Context injection with maximum priority",
            "Natural message to customer",
            "Multi-handover history preserved"
          ]
        },
        "failClosed": {
          "title": "Fail-Closed Resilience",
          "intro": "When Redis is unavailable, the system blocks AI (does not go silent):",
          "comment": "# Fail-closed",
          "customerReceives": "# Customer receives: One moment, we are transferring you...",
          "notSilent": "# NOT: total silence (fail-open)",
          "retryStrategy": "Retry Strategy",
          "retryStrategyDesc": "3 attempts with exponential backoff (50ms base) + random jitter"
        },
        "websocket": {
          "title": "WebSocket Real-Time",
          "intro": "Event broadcasting for frontend synchronization:",
          "conversationUpdate": "conversation_update",
          "conversationUpdateDesc": "New conversation or status change",
          "newMessage": "new_message",
          "newMessageDesc": "Normalized client/AI message",
          "operatorMessage": "operator_message",
          "operatorMessageDesc": "Operator message (WhatsApp/Web)",
          "cacheInvalidation": "cache_invalidation",
          "cacheInvalidationDesc": "Refetch trigger in frontend"
        },
        "idempotency": {
          "title": "Idempotency Gate",
          "intro": "Protection against duplicate processing:",
          "step1": "Request arrives with",
          "step2": "Check Redis: already processed? -> return cached response",
          "step3": "Slot reserved -> process request",
          "step4": "Response cached with 24h TTL"
        }
      },
      "architecture": {
        "title": "Architecture",
        "gatewayEnterprise": "(Gateway Enterprise)",
        "rateLimiter": "Rate Limiter",
        "multiTier": "(Multi-tier)",
        "circuitBreaker": "Circuit Breaker",
        "threeStates": "(3 states)",
        "handoverGate": "Handover Gate",
        "failClosed": "(Fail-closed)",
        "chatOrchestrator": "Chat Orchestrator",
        "idempotency": "+ Idempotency",
        "aiEngine": "AI Engine",
        "langGraph": "(LangGraph)",
        "memoryEngine": "Memory Engine",
        "postgresql": "(PostgreSQL)",
        "websocketBroadcast": "WebSocket Broadcast",
        "requestFlow": "Request Flow:",
        "step1": "1. Rate Limit Check (per-tenant tier)",
        "step2": "2. Circuit Breaker (protect downstream)",
        "step3": "3. Handover Gate (AI blocking)",
        "step4": "4. Idempotency Check (dedupe)",
        "step5": "5. AI Engine Call (with timeout)",
        "step6": "6. WebSocket Broadcast (real-time)",
        "step7": "7. Cache Response (24h TTL)"
      },
      "decisions": {
        "title": "Technical Decisions",
        "failClosed": {
          "question": "Why fail-closed and not fail-open?",
          "answer": "In customer service, silence is worse than a fallback response. Fail-closed ensures the customer always receives feedback (we are transferring you) even when Redis is unavailable. Fail-open would leave the customer waiting indefinitely."
        },
        "redisLatch": {
          "question": "Why Redis latch and not database flag?",
          "answer": "Redis latch with automatic TTL eliminates the need for cleanup workers. If the operator forgets to return the conversation, the TTL expires and AI resumes automatically. Database flags would need cron jobs for timeout."
        },
        "slidingWindow": {
          "question": "Why sliding window and not fixed window?",
          "answer": "Fixed window has the boundary burst problem: 100 requests at second 59 + 100 at second 0 = 200 requests in 2 seconds. Sliding window distributes uniformly and avoids this spike."
        },
        "luaScripts": {
          "question": "Why Lua scripts for token bucket?",
          "answer": "Token bucket needs atomic operations (read-modify-write). Without Lua, we would have race conditions between GET and SET. Lua scripts execute atomically on the Redis server, guaranteeing consistency even with thousands of concurrent requests."
        }
      },
      "nextProject": "Next: Memory Engine ->"
    },
    "rules": {
      "badge": "Optimus Platform",
      "badgeSecondary": "Business Intelligence",
      "title": "Rules Engine",
      "description": "Python-native rules engine that eliminates JSONLogic interpreted overhead. Sub-millisecond evaluation, runtime rules created per tenant, complete type safety.",
      "tags": {
        "pythonLambda": "Python Lambda",
        "subMillisecond": "Sub-millisecond",
        "multiTenant": "Multi-tenant",
        "eventDriven": "Event-driven",
        "typeSafe": "Type Safe",
        "cacheCoherence": "Cache Coherence"
      },
      "problem": {
        "title": "The Problem: Why JSONLogic Does Not Scale",
        "jsonLogicTrap": "JSONLogic - The Trap",
        "simpleRule": "// Simple rule in JSONLogic",
        "issues": {
          "interpreted": "Interpreted recursively",
          "interpretedDesc": "each operator is a nested function call",
          "limitedOps": "Limited operators",
          "limitedOpsDesc": "only supports {'<'}, {'>'}, ==, in, and, or",
          "noTypeCheck": "No type checking",
          "noTypeCheckDesc": "errors only appear at runtime",
          "debugImpossible": "Impossible to debug",
          "debugImpossibleDesc": "incomprehensible stack traces",
          "slowPerf": "~50-200ms",
          "slowPerfDesc": "to evaluate 100 complex rules"
        },
        "pythonNativeSolution": "Python-Native - The Solution",
        "sameRule": "# Same rule in Python-native",
        "benefits": {
          "compiledOnce": "Compiled once",
          "compiledOnceDesc": "native Python bytecode",
          "fullPower": "Full Python power",
          "fullPowerDesc": "regex, datetime, math, everything",
          "typeHints": "Type hints + mypy",
          "typeHintsDesc": "errors before deploy",
          "normalDebug": "Normal debugging",
          "normalDebugDesc": "pdb, breakpoints, stack traces",
          "fastPerf": "<0.5ms",
          "fastPerfDesc": "to evaluate 100 rules - 1000x faster"
        },
        "benchmark": {
          "title": "Real Benchmark: JSONLogic vs Python-Native",
          "jsonLogic": "JSONLogic (100 rules)",
          "pythonNative": "Python-Native (100 rules)",
          "fasterCold": "Faster (cold)",
          "fasterCached": "Faster (cached)"
        }
      },
      "architecture": {
        "title": "Architecture: Rules Engine + Coordinator",
        "requestFlow": "Request Flow (sub-ms target)",
        "clientRequest": "Client Request",
        "backendOrchestrator": "Backend Orchestrator",
        "rulesCoordinator": "Rules Coordinator",
        "cacheFallback": "(Cache + Fallback)",
        "rulesEngine": "Rules Engine",
        "port": "(Port 8040)",
        "redisCache": "Redis Cache",
        "ttl15min": "(15 min)",
        "postgresql": "PostgreSQL",
        "rulesDb": "(Rules DB)",
        "fallbackHierarchy": "Fallback Hierarchy (never fails)",
        "primary": "1. Rules Engine API -> Primary (target <50ms)",
        "secondary": "2. Redis Cache      -> Secondary (target <5ms)",
        "basic": "3. Basic Fallback   -> Always available (keyword-based)",
        "components": {
          "rulesEngine": {
            "title": "Rules Engine",
            "desc": "Dedicated microservice that compiles and executes Python-native rules. Each tenant has their own isolated rules.",
            "items": [
              "Python lambda compilation",
              "Execution sandboxing",
              "Per-rule metrics",
              "Multi-vertical support"
            ]
          },
          "rulesCoordinator": {
            "title": "Rules Coordinator",
            "desc": "Intelligent proxy in Backend Orchestrator with cache, circuit breaker and multi-tier fallback.",
            "items": [
              "Evaluation cache (15min TTL)",
              "Memory context enrichment",
              "Circuit breaker protection",
              "Graceful degradation"
            ]
          },
          "contextEnrichment": {
            "title": "Context Enrichment",
            "desc": "Integration with Memory Engine to enrich facts with customer historical context.",
            "items": [
              "Interaction frequency",
              "Detected urgency level",
              "Pain/emergency indicators",
              "Intelligent patterns"
            ]
          }
        }
      },
      "runtimeCreation": {
        "title": "Runtime Rule Creation: Every Business is Unique",
        "intro": "The Rules Engine key differentiator is allowing each tenant to create their own rules in real-time, without deploy, without downtime, without code.",
        "apiTitle": "Rule Creation API",
        "examples": {
          "dental": {
            "title": "Dental Rule",
            "desc": "Cleaning reminder based on last visit + insurance status. If 6 months passed and has coverage -> schedule preventive."
          },
          "ecommerce": {
            "title": "E-commerce Rule",
            "desc": "Cart abandoned for 2h + value > $200 + recurring customer -> 10% discount offer + free shipping."
          },
          "legal": {
            "title": "Legal Rule",
            "desc": "Court deadline in 48h + customer did not respond to last message -> urgent alert + escalation to responsible attorney."
          }
        },
        "multiTenant": {
          "title": "Complete Multi-tenant Isolation",
          "clinic": {
            "name": "Clinic ABC",
            "rules": "47 active rules",
            "vertical": "Vertical: dental",
            "focus": "Focus: scheduling"
          },
          "store": {
            "name": "Store XYZ",
            "rules": "89 active rules",
            "vertical": "Vertical: e-commerce",
            "focus": "Focus: conversion"
          },
          "law": {
            "name": "Law Firm 123",
            "rules": "23 active rules",
            "vertical": "Vertical: legal",
            "focus": "Focus: deadlines"
          },
          "footer": "Each tenant has completely isolated rules. No rule from Clinic ABC affects Store XYZ. Zero business logic leakage between clients."
        }
      },
      "deepDive": {
        "title": "Deep Dive: How It Works",
        "coordinator": {
          "title": "Rules Coordinator: Cache + Intelligent Fallback",
          "docstring": "Central Rules Coordinator",
          "responsibilities": [
            "1. Intelligent proxy for Rules Engine with <50ms target",
            "2. Redis cache for optimized performance",
            "3. Integration with Memory Coordinator for enriched context",
            "4. Fallback when Rules Engine fails",
            "5. Circuit breaker for failure protection"
          ],
          "comment1": "# 1. Enrich data with Memory Coordinator",
          "comment2": "# 2. Try Rules Engine (primary)",
          "comment3": "# Cache for future reuse",
          "comment4": "# 3. Fallback to Redis cache",
          "comment5": "# 4. Basic fallback (keyword-based, never fails)",
          "responsibilities[0]": "1. Proxy inteligente para Rules Engine com &lt;50ms target",
          "responsibilities[1]": "2. Cache Redis para performance otimizada",
          "responsibilities[2]": "3. Integra√ß√£o com Memory Coordinator para contexto enriquecido",
          "responsibilities[3]": "4. Fallback quando Rules Engine falha",
          "responsibilities[4]": "5. Circuit breaker para prote√ß√£o contra falhas"
        },
        "enrichment": {
          "title": "Context Enrichment: Rules with Historical Context",
          "docstring": "Extracts intelligent patterns from memory context for more sophisticated rules",
          "comments": {
            "detectsUrgency": "# Detects urgency (pain, emergency, bleeding)",
            "detectsScheduling": "# Detects scheduling need",
            "analyzesFrequency": "# Analyzes interaction frequency",
            "result": "# Result: rules can use enriched facts"
          }
        },
        "eventTypes": {
          "title": "Event-Driven: Automatic Triggers",
          "messageReceived": "message_received",
          "messageReceivedDesc": "New customer message",
          "conversationStarted": "conversation_started",
          "conversationStartedDesc": "Conversation start",
          "handoverCompleted": "handover_completed",
          "handoverCompletedDesc": "Agent finished",
          "appointmentScheduled": "appointment_scheduled",
          "appointmentScheduledDesc": "Appointment confirmed",
          "cartAbandoned": "cart_abandoned",
          "cartAbandonedDesc": "Cart abandoned",
          "deadlineApproaching": "deadline_approaching",
          "deadlineApproachingDesc": "Deadline approaching",
          "sentimentNegative": "sentiment_negative",
          "sentimentNegativeDesc": "Unsatisfied customer",
          "timeBased": "time_based",
          "timeBasedDesc": "Time-based trigger"
        }
      },
      "results": {
        "title": "Results: Rules That Scale",
        "latencyP95": "P95 Latency",
        "latencyP95Desc": "Performance target",
        "vsJsonLogic": "vs JSONLogic",
        "vsJsonLogicDesc": "With warm cache",
        "rulesInProduction": "Rules in Production",
        "rulesInProductionDesc": "Dental + E-commerce + Medical",
        "evaluationUptime": "Evaluation Uptime",
        "evaluationUptimeDesc": "Fallback never fails",
        "decisions": {
          "title": "Key Technical Decisions",
          "pythonLambda": {
            "title": "Python Lambda vs Custom DSL",
            "desc": "We considered creating a DSL (Domain-Specific Language) for rules, but decided to use Python lambda directly. Reason: developers already know Python, normal debugging, type hints work, entire ecosystem available. Sandboxing is done via AST parsing + restricted builtins."
          },
          "cache15min": {
            "title": "15-minute cache (not infinite)",
            "desc": "Rules are cached for 15 minutes, not infinitely. This allows rule changes (via API) to be reflected in reasonable time without manual invalidation. The tradeoff between performance and freshness was calibrated in production."
          },
          "keywordFallback": {
            "title": "Keyword-Based Fallback (Always Works)",
            "desc": "The last fallback level uses simple keyword analysis. Not sophisticated, but guarantees the system NEVER fails to evaluate a message. Pain -> urgency, schedule -> appointment. Simple but functional as a last resort."
          },
          "separateMicroservice": {
            "title": "Separate Microservice (not library)",
            "desc": "Rules Engine is an independent microservice, not an imported library. This allows horizontal scaling, independent deploy, and failure isolation. If Rules Engine crashes, Coordinator uses cache/fallback."
          }
        }
      },
      "technicalStack": {
        "title": "Technical Stack",
        "runtime": "Runtime",
        "runtimeDesc": "Python 3.11+ (bytecode optimized)",
        "framework": "Framework",
        "frameworkDesc": "FastAPI + Pydantic",
        "cache": "Cache",
        "cacheDesc": "Redis (cache + pub/sub)",
        "storage": "Storage",
        "storageDesc": "PostgreSQL (rules metadata)",
        "isolation": "Isolation",
        "isolationDesc": "Per-tenant rule namespaces",
        "circuitBreaker": "Circuit Breaker",
        "circuitBreakerDesc": "5 failures -> 60s recovery",
        "httpClient": "HTTP Client",
        "httpClientDesc": "HTTPX async pooling",
        "metrics": "Metrics",
        "metricsDesc": "Prometheus + per-rule tracking"
      },
      "cta": {
        "title": "Want to discuss more about Rules Engines?",
        "desc": "JSONLogic vs Python-native, custom DSLs, or how to make rules scale - I love talking about these topics.",
        "contact": "Get in Touch"
      },
      "meta": {
        "title": "Rules Engine - Python-Native 1000x faster than JSONLogic | Marcelo Marleta",
        "description": "Python-native rules engine that eliminates JSONLogic. Sub-millisecond evaluation, runtime rules per tenant, complete type safety."
      }
    },
    "whatsapp": {
      "backToPortfolio": "Back to Portfolio",
      "badge": "Messaging Infrastructure",
      "title": "WhatsApp Integration",
      "description": "Enterprise WhatsApp service with intelligent Message Aggregator, isolated Redis, graceful degradation, and bidirectional Evolution API integration.",
      "tags": {
        "aggregator": "Message Aggregator",
        "debounce": "5s Debounce",
        "evolution": "Evolution API",
        "redis": "Isolated Redis",
        "degradation": "Graceful Degradation",
        "websocket": "WebSocket"
      },
      "problem": {
        "title": "The Problem: WhatsApp \"Split Messages\"",
        "naturalBehavior": "‚ùå Natural User Behavior",
        "userFragmented": "Users send fragmented thoughts across multiple quick messages. This is normal behavior on WhatsApp.",
        "normalBehavior": "Users send fragmented thoughts across multiple quick messages. This is normal behavior on WhatsApp.",
        "inWhatsApp": "Users send fragmented thoughts across multiple quick messages. This is normal behavior on WhatsApp.",
        "withoutAggregation": "Without Aggregation (chaos):",
        "issue1": "5 AI API calls for ONE intent",
        "issue2": "5 confusing replies (\"Hi!\", \"Good!\", \"Schedule what?\"...)",
        "issue3": "5x higher token cost",
        "issue4": "Horrible UX for the customer"
      },
      "solution": {
        "title": "‚úÖ Message Aggregator (5s Debounce)",
        "messagesReceived": "// Messages received in 14s...",
        "aggregatedIn": "// Aggregated into ONE message:",
        "aggregatedResult": "\"Hi, how are you? I'd like to schedule an appointment for tomorrow at 10am\"",
        "description": "The Message Aggregator waits 5 seconds after each message.",
        "withAggregation": "With Aggregation (clean):",
        "benefit1": "1 AI API call with full context",
        "benefit2": "1 accurate reply (\"Perfect! I scheduled for tomorrow 10am.\")",
        "benefit3": "5x lower token cost",
        "benefit4": "Natural, fluid UX",
        "extendsTimeout": "extends the timeout"
      },
      "aggregatorFlow": {
        "title": "üìä Message Aggregator Flow",
        "timeline": "Message Aggregator Timeline",
        "message": "Message",
        "timeout": "Timeout",
        "send": "SEND",
        "timer": "5s timer",
        "reset": "RESET!",
        "timerExtended": "Timer extended",
        "timerExpired": "Timer expired",
        "aggregatedMessage": "Aggregated Message:",
        "backendOrch": "‚Üí Backend Orch.",
        "aiEngine": "‚Üí AI Engine"
      },
      "typingDetection": {
        "title": "üîÑ Typing Detection: The Secret to Fluidity",
        "description": "WhatsApp sends \"typing...\" events via webhook. The Message Aggregator",
        "realScenario": "Real Scenario:",
        "step1": "User sends \"Hi\" ‚Üí 5s timer starts",
        "step2": "Webhook: \"user typing\" ‚Üí timer paused",
        "step3": "User sends \"I want to schedule\" ‚Üí 5s timer restarts",
        "step4": "No typing, timer expires ‚Üí aggregated message sent",
        "result": "Result: The user can type at their natural pace. The system \"waits\" intelligently until they finish the full thought.",
        "safetyLimits": "Safety Limits",
        "maxTimeout": "Max Total Timeout",
        "maxMessages": "Max Messages per Aggregation",
        "debounceBase": "Debounce Base",
        "limitsDesc": "These limits prevent DoS and ensure messages don't get \"stuck\" indefinitely if the user keeps typing.",
        "extendIntelligently": "extend the timeout intelligently",
        "resultDesc": "The user can type at their natural pace. The system \"waits\" intelligently until they finish the full thought."
      },
      "redisIsolated": {
        "title": "üîí Isolated Redis: Traffic Separation",
        "description": "WhatsApp generates <strong class=\"text-green-400\">a lot of webhook traffic</strong> -",
        "mixingBad": "Mixing this with the main Redis is a recipe for disaster.",
        "archTitle": "REDIS ISOLATED ARCHITECTURE",
        "mainRedis": "üî¥ Main Redis (Port 6379/6380)",
        "aiEngineCache": "AI Engine ‚úÖ Conversation cache",
        "orchestratorCache": "Backend Orchestrator ‚úÖ Rate limiting, cache",
        "memoryCache": "Memory Engine ‚úÖ Hot/Warm storage",
        "rulesCache": "Rules Engine ‚úÖ Rules cache",
        "celeryQueues": "Celery Workers ‚úÖ Task queues",
        "whatsappRedis": "üü¢ WhatsApp Redis (Port 6382) - ISOLATED",
        "whatsappOnly": "WhatsApp Integration ONLY",
        "messageBuffers": "Message buffers (aggregation)",
        "typingState": "Typing state tracking",
        "webhookDedupe": "Webhook deduplication",
        "instanceMapping": "Instance ‚Üí Tenant mapping",
        "deliveryCache": "Delivery status cache",
        "benefits": "‚ö° Benefits:",
        "benefit1": "‚Ä¢ WhatsApp broadcast doesn't affect conversation cache",
        "benefit2": "‚Ä¢ WhatsApp Redis failure doesn't bring down the core system",
        "benefit3": "‚Ä¢ Separate metrics for debugging",
        "benefit4": "‚Ä¢ Independent scaling if needed",
        "faultIsolation": "üõ°Ô∏è",
        "faultIsolationTitle": "Fault Isolation",
        "faultIsolationDesc": "If WhatsApp Redis goes down or slows, the core system (AI, Memory, Rules) keeps working normally.",
        "separateMetrics": "üìä",
        "separateMetricsTitle": "Separate Metrics",
        "separateMetricsDesc": "Independent monitoring: WhatsApp Redis latency doesn't pollute core system metrics. Easier debugging.",
        "independentScale": "üìà",
        "independentScaleTitle": "Independent Scaling",
        "independentScaleDesc": "High WhatsApp demand? Scale only WhatsApp Redis. No need to touch the core infra.",
        "highWebhookTraffic": "a lot of webhook traffic"
      },
      "gracefulDegradation": {
        "title": "‚ö° Graceful Degradation: A System That Doesn't Go Down",
        "description": "Under high load, the system degrades non-essential features to keep the core running. Configurable degradation levels.",
        "degradesNonEssential": "Under high load, the system degrades non-essential features to keep the core running. Configurable degradation levels.",
        "toKeepCore": "Under high load, the system degrades non-essential features to keep the core running. Configurable degradation levels.",
        "normal": "NORMAL",
        "normalUsers": "Users &lt; 500",
        "normalFeature1": "‚úÖ Full aggregation (5s)",
        "normalFeature2": "‚úÖ Typing detection active",
        "normalFeature3": "‚úÖ WebSocket real-time",
        "normalFeature4": "‚úÖ All features",
        "degradedLow": "DEGRADED_LOW",
        "degradedLowUsers": "500-2000 users",
        "degradedLowFeature1": "‚ö†Ô∏è Reduced aggregation (3s)",
        "degradedLowFeature2": "‚ö†Ô∏è Limited typing detection",
        "degradedLowFeature3": "‚úÖ WebSocket active",
        "degradedLowFeature4": "‚ö†Ô∏è Reduced metrics",
        "degradedHigh": "DEGRADED_HIGH",
        "degradedHighUsers": "&gt; 2000 users",
        "degradedHighFeature1": "‚ùå Minimal aggregation (1s)",
        "degradedHighFeature2": "‚ùå Typing detection off",
        "degradedHighFeature3": "‚ö†Ô∏è WebSocket pooled",
        "degradedHighFeature4": "‚ùå Critical metrics only",
        "configurableThresholds": "Configurable Thresholds",
        "degradationBasedOn": "Degradation based on:",
        "pendingUsers": "Number of pending users",
        "redisLatency": "Redis latency",
        "webhookErrorRate": "Webhook error rate",
        "messageQueueSize": "Message queue size"
      },
      "deterministicId": {
        "title": "üéØ 1 Conversation per Phone (Deterministic ID)",
        "description": "On WhatsApp, each phone = one conversation. It doesn't matter which header arrives,",
        "forSamePhone": "This eliminates parallel sessions and cross-user pollution.",
        "deterministicCalc": "Deterministic Calculation",
        "example": "Example:",
        "alwaysSameUuid": "ALWAYS the same UUID for the same inputs",
        "ignoresHeaders": "Ignores external headers completely",
        "headersIgnored": "X-Conversation-ID headers are <strong class=\"text-red-400\">ignored</strong>.",
        "phoneNormalization": "Phone Normalization",
        "allBecomeSameUuid": "// All become the SAME UUID:",
        "normalizedBefore": "// Phone is normalized before hashing:",
        "removeSpaces": "// 1. Remove spaces, hyphens, parentheses",
        "removePrefix": "// 2. Remove \"+\" prefix",
        "ensureNumeric": "// 3. Ensure pure numeric format",
        "noMatterHow": "No matter how the phone arrives (WhatsApp, Evolution, manual), normalization guarantees consistency.",
        "alwaysCalculates": "always computes the same UUID",
        "ignored": "ignored",
        "preventsHijack": ". Prevents a malicious client from \"jumping\" to another user's conversation."
      },
      "handoverIntegration": {
        "title": "üë• Handover Integration",
        "description": "When a human agent takes over the conversation, the behavior of the",
        "duringHandover": "During Handover:",
        "aggregationPaused": "‚è∏Ô∏è",
        "aggregationPausedTitle": "Aggregation Paused",
        "aggregationPausedDesc": "Customer messages go directly to the operator, with no 5s delay.",
        "immediateDelivery": "‚ö°",
        "immediateDeliveryTitle": "Immediate Delivery",
        "immediateDeliveryDesc": "Operator replies are sent instantly.",
        "websocketRealtime": "üîÑ",
        "websocketRealtimeTitle": "WebSocket Real-time",
        "websocketRealtimeDesc": "Operator frontend receives messages in real time.",
        "handoverConfig": "Handover Configuration",
        "supportRealtime": "support real-time interaction"
      },
      "evolutionApi": {
        "title": "üöÄ Evolution API Integration",
        "multiTenantInstances": "Multi-tenant Instances",
        "multiTenantDesc": "Each tenant can have multiple WhatsApp instances. The system automatically resolves which tenant \"owns\" each instance via O(1) cache.",
        "instanceMapping": "// Instance ‚Üí Tenant mapping",
        "lookupO1": "// Lookup O(1) via Redis:",
        "webhookSecurity": "Webhook Security",
        "webhookSecurityDesc": "API keys are validated via HMAC with a dedicated secret. The API key hash (not the key itself) is cached for fast lookups.",
        "validationFlow": "// API Key validation flow:",
        "step1": "1. Webhook arrives with X-API-Key header",
        "step2": "2. HMAC(apikey, EVOLUTION_SECRET) ‚Üí hash",
        "step3": "3. Redis lookup: apikeyhash:tenant:{hash}",
        "step4": "4. If match ‚Üí authorized",
        "step5": "5. If not ‚Üí schema scan (fallback)",
        "audioSupport": "üé§ Audio Message Support",
        "voiceNotes": "üéôÔ∏è",
        "voiceNotesTitle": "Voice Notes",
        "voiceNotesDesc": "Customer sends audio ‚Üí Audio Processor transcribes ‚Üí Text goes to AI Engine",
        "audioReply": "üîä",
        "audioReplyTitle": "Audio Reply",
        "audioReplyDesc": "AI response ‚Üí TTS (Text-to-Speech) ‚Üí Audio sent to the customer",
        "minioStorage": "üìÅ",
        "minioStorageTitle": "MinIO Storage",
        "minioStorageDesc": "Audio files stored in MinIO for async processing"
      },
      "results": {
        "title": "Production Results",
        "aiReduction": "~80%",
        "aiReductionLabel": "AI Call Reduction",
        "aiReductionDesc": "Via message aggregation",
        "webhookLatency": "&lt;100ms",
        "webhookLatencyLabel": "Webhook Latency",
        "webhookLatencyDesc": "P95 end-to-end",
        "redisIsolation": "100%",
        "redisIsolationLabel": "Redis Isolation",
        "redisIsolationDesc": "Zero cross-pollution",
        "simultaneousUsers": "2000+",
        "simultaneousUsersLabel": "Simultaneous Users",
        "simultaneousUsersDesc": "Before degradation",
        "techStack": "Technical Stack",
        "framework": "Framework",
        "storage": "Storage",
        "realtime": "Real-time",
        "whatsappApi": "WhatsApp API",
        "audio": "Audio",
        "observability": "Observability",
        "rateLimiting": "Rate Limiting",
        "httpClient": "HTTP Client"
      },
      "cta": {
        "title": "Interested in WhatsApp integrations?",
        "description": "Message aggregation, webhooks, multi-tenant instances ‚Äî I have hands-on experience with real-world challenges.",
        "contact": "Contact"
      }
    },
    "audio": {
      "backToPortfolio": "Back to Portfolio",
      "badge": "Media Processing",
      "title": "Audio Processor",
      "description": "Audio processing microservice with multi-provider STT/TTS, async processing via Celery, MinIO storage, and intelligent fallback between providers.",
      "tags": {
        "multiProvider": "Multi-Provider",
        "groqWhisper": "Groq Whisper",
        "elevenLabs": "ElevenLabs",
        "celery": "Celery",
        "minio": "MinIO",
        "formats": "8 Formats"
      },
      "challenge": {
        "title": "The Challenge: Audio in Enterprise Chatbots",
        "voiceNotes": "üé§ WhatsApp Voice Notes",
        "voiceNotesDesc": "WhatsApp users love sending voice notes. In some segments (clinics,",
        "issue1": "Long audios (1‚Äì3 minutes) with multiple pieces of information",
        "issue2": "Regional accents, background noise, slang",
        "issue3": "Expectation of audio responses as well",
        "commonProblems": "‚ö†Ô∏è Common Problems",
        "problem1": "Single provider",
        "problem1Desc": "- if it goes down, the system stops",
        "problem2": "Synchronous processing",
        "problem2Desc": "- blocks the thread",
        "problem3": "No fallback",
        "problem3Desc": "- error = lost message",
        "problem4": "Fixed cost",
        "problem4Desc": "- same provider for everything",
        "problem5": "Limited formats",
        "problem5Desc": "- only MP3/WAV",
        "audioPercentage": "more than 40% of messages are audio"
      },
      "architecture": {
        "title": "Multi-Provider Architecture",
        "diagramTitle": "Audio Processor Architecture",
        "whatsappVoice": "WhatsApp Voice Note",
        "webMobileAudio": "Web/Mobile Audio",
        "textResponse": "Text Response",
        "stt": "STT",
        "tts": "TTS",
        "primary": "Primary",
        "fallback": "Fallback",
        "storage": "Storage",
        "workers": "Workers",
        "sttTitle": "üé§ STT (Speech-to-Text)",
        "groqWhisperTitle": "Groq Whisper",
        "groqWhisperDesc": "Whisper Large v3 via Groq API. Extremely fast (dedicated hardware), excellent for Brazilian Portuguese.",
        "openaiWhisperTitle": "OpenAI Whisper",
        "openaiWhisperDesc": "OpenAI official API. Slower than Groq, but extremely reliable as backup.",
        "otherProvidersTitle": "Google / Azure / AWS",
        "available": "Available",
        "otherProvidersDesc": "Additional providers configurable for specific cases or compliance requirements.",
        "ttsTitle": "üîä TTS (Text-to-Speech)",
        "elevenLabsTitle": "ElevenLabs",
        "elevenLabsDesc": "Model: eleven_multilingual_v2. Ultra-realistic voices, excellent intonation in Portuguese.",
        "openaiTtsTitle": "OpenAI TTS",
        "openaiTtsDesc": "Model: tts-1-hd, Voice: shimmer. High quality, good backup when ElevenLabs is unavailable.",
        "edgeTtsTitle": "Edge TTS",
        "freeTier": "Free Tier",
        "edgeTtsDesc": "Voice: pt-BR-FranciscaNeural. Free (Microsoft Edge), used for development/testing or emergency fallback."
      },
      "asyncProcessing": {
        "title": "‚ö° Async Processing with Celery",
        "description": "Audio is heavy. A 2-minute voice note can take 5‚Äì10 seconds to transcribe. Processing synchronously would block the server. The solution: dedicated Celery workers.",
        "wouldBlock": "Audio is heavy. A 2-minute voice note can take 5‚Äì10 seconds to transcribe. Processing synchronously would block the server. The solution: dedicated Celery workers.",
        "step1Icon": "üì•",
        "step1Title": "1. Receive Audio",
        "step1Desc": "API receives the audio file (any format), validates and stores it in MinIO. Returns job_id immediately.",
        "step2Icon": "‚öôÔ∏è",
        "step2Title": "2. Celery Processes",
        "step2Desc": "Worker pulls from the queue, downloads from MinIO, converts format if needed, sends to the STT provider, saves the result.",
        "step3Icon": "üì§",
        "step3Title": "3. Callback/Poll",
        "step3Desc": "Result available via polling (GET /status/job_id) or webhook callback when configured.",
        "flowTitle": "Transcription Flow",
        "clientSends": "# 1. Client sends audio",
        "immediateResponse": "# Immediate response (~50ms)",
        "workerProcesses": "# 2. Celery worker processes in background",
        "downloadingFrom": "[Worker] Downloading from MinIO:",
        "converting": "[Worker] Converting OGG ‚Üí WAV (ffmpeg)",
        "sendingTo": "[Worker] Sending to Groq Whisper...",
        "savingResult": "[Worker] Saving result to Redis",
        "clientChecks": "# 3. Client checks status",
        "solution": ". The solution: dedicated Celery workers.",
        "transcriptionComplete": "[Worker] Transcription complete: \"Hi, I want to schedule an appointment...\""
      },
      "formats": {
        "title": "üéµ 8 Supported Audio Formats",
        "description": "WhatsApp uses OGG/OPUS. Browsers use WebM. iPhones use M4A. The Audio Processor accepts any of them and converts internally via FFmpeg.",
        "mp3": "MP3",
        "mp3Desc": "Most common",
        "wav": "WAV",
        "wavDesc": "Uncompressed",
        "ogg": "OGG",
        "oggDesc": "WhatsApp Android",
        "opus": "OPUS",
        "opusDesc": "WhatsApp codec",
        "m4a": "M4A",
        "m4aDesc": "iPhone/AAC",
        "flac": "FLAC",
        "flacDesc": "Lossless",
        "aac": "AAC",
        "aacDesc": "High quality",
        "wma": "WMA",
        "wmaDesc": "Windows legacy",
        "autoConversion": "Automatic Conversion",
        "autoConversionDesc": "The system detects the format automatically (magic bytes, not extension). If the STT provider doesn't support the format, it converts to WAV via FFmpeg before sending. Everything is transparent to the caller."
      },
      "minio": {
        "title": "üì¶ Storage with MinIO",
        "whyMinio": "Why MinIO?",
        "s3Compatible": "S3-compatible",
        "s3CompatibleDesc": "- same API as AWS S3, no vendor lock-in",
        "selfHosted": "Self-hosted",
        "selfHostedDesc": "- data stays under your control",
        "highPerformance": "High performance",
        "highPerformanceDesc": "- optimized for medium-sized files",
        "webConsole": "Web console",
        "webConsoleDesc": "- UI for debug and management",
        "configuration": "Configuration",
        "managementConsole": "# Management console",
        "lifecycle": "File Lifecycle",
        "upload": "üì•",
        "uploadTitle": "Upload",
        "uploadDesc": "Client sends",
        "store": "üíæ",
        "storeTitle": "Store",
        "storeDesc": "MinIO stores",
        "process": "‚öôÔ∏è",
        "processTitle": "Process",
        "processDesc": "Worker downloads, processes",
        "cleanup": "üóëÔ∏è",
        "cleanupTitle": "Cleanup",
        "cleanupDesc": "TTL expires, deletes"
      },
      "fallback": {
        "title": "üîÑ Intelligent Fallback",
        "description": "AI providers fail. Groq may spike latency, ElevenLabs may be in maintenance. The system automatically tries the next provider in the fallback chain.",
        "sttChain": "# STT fallback chain",
        "sttPrimary": "# Primary",
        "groqFailed": "Groq failed, falling back to OpenAI",
        "allSttFailed": "All STT providers failed",
        "ttsChain": "# TTS fallback chain",
        "ttsPrimary": "# Primary",
        "elevenLabsFailed": "ElevenLabs failed, falling back to OpenAI",
        "paidTtsFailed": "Paid TTS failed, using free Edge TTS",
        "alwaysAvailable": "# Always available",
        "timeoutIcon": "‚è±Ô∏è",
        "timeoutTitle": "Timeout Detection",
        "timeoutDesc": "If a provider doesn't respond within 30s, assume failure and try the next.",
        "quotaIcon": "üí∞",
        "quotaTitle": "Quota Handling",
        "quotaDesc": "Rate limit or quota exceeded? Automatic fallback without losing the message.",
        "freeIcon": "üÜì",
        "freeTitle": "Free Tier Backup",
        "freeDesc": "Edge TTS is free and always available as a last resort."
      },
      "tenantConfig": {
        "title": "üè¢ Per-tenant Configuration",
        "description": "Each tenant can have custom TTS settings: different voice, preferred provider, speech speed. Stored in Memory Engine and loaded at runtime.",
        "dentalClinic": "Example: Dental Clinic",
        "ecommerce": "Example: E-commerce"
      },
      "whatsappIntegration": {
        "title": "üì± WhatsApp Integration",
        "diagramTitle": "WhatsApp Voice Note Flow",
        "clientSendsAudio": "Client sends audio",
        "audioProcessor": "Audio Processor",
        "aiEngine": "AI Engine",
        "evolutionApi": "Evolution API",
        "whatsappIntegration": "WhatsApp Integration",
        "celeryProcesses": "[Celery processes]",
        "clientReceivesAudio": "Client receives audio",
        "responseMode": "Configurable Response Mode",
        "modeAudio": "audio",
        "modeAudioDesc": "Always responds in audio. Ideal for users who prefer listening.",
        "modeText": "text",
        "modeTextDesc": "Always responds in text. Saves TTS cost.",
        "modeMatch": "match",
        "modeMatchDesc": "Responds in the same format received (audio ‚Üí audio, text ‚Üí text)."
      },
      "results": {
        "title": "Results",
        "uptime": "99.5%",
        "uptimeLabel": "Transcription Uptime",
        "uptimeDesc": "With multi-provider fallback",
        "transcriptionTime": "&lt;3s",
        "transcriptionTimeLabel": "Transcription (1min audio)",
        "transcriptionTimeDesc": "Groq Whisper P95",
        "formatsCount": "8",
        "formatsCountLabel": "Supported Formats",
        "formatsCountDesc": "Automatic conversion",
        "providersCount": "5",
        "providersCountLabel": "TTS Providers",
        "providersCountDesc": "Including free tier",
        "techStack": "Technical Stack",
        "framework": "Framework",
        "taskQueue": "Task Queue",
        "storage": "Storage",
        "audioProcessing": "Audio Processing",
        "sttPrimary": "STT Primary",
        "ttsPrimary": "TTS Primary",
        "ttsBackup": "TTS Backup",
        "ttsFree": "TTS Free"
      },
      "cta": {
        "title": "Need audio processing?",
        "description": "STT, TTS, format conversion, chatbot integration ‚Äî I have experience with production challenges.",
        "contact": "Contact"
      }
    },
    "observability": {
      "backToPortfolio": "‚Üê Back",
      "badge": "Enterprise Observability",
      "title": "Observability Stack",
      "description": "Complete observability system with business metrics, distributed tracing, token cost monitoring, AI decision telemetry, and real-time alerts.",
      "metrics": {
        "meltPillars": "4",
        "meltPillarsLabel": "MELT pillars",
        "customMetrics": "50+",
        "customMetricsLabel": "Custom Metrics",
        "traceCoverage": "100%",
        "traceCoverageLabel": "Trace Coverage",
        "mttrDebug": "&lt;5min",
        "mttrDebugLabel": "MTTR Debug"
      },
      "whyObservability": {
        "intro": "In conversational AI systems, \"it didn't work\" isn't a diagnosis. We need to answer specific questions:",
        "debug": "Debug",
        "debugQ": ": Why didn't the AI understand \"I want to book tomorrow\"?",
        "performance": "Performance",
        "performanceQ": ": Which LangGraph node is slow?",
        "costs": "Costs",
        "costsQ": ": How much are we spending on tokens per tenant?",
        "business": "Business",
        "businessQ": ": What % of conversations are resolved without a human?",
        "quality": "Quality",
        "qualityQ": ": Is RAG returning correct prices?",
        "title": "Why Observability Is Critical"
      },
      "stack": {
        "businessMetrics": {
          "title": "üìä Business Metrics (Prometheus)",
          "description": "Business metrics with clear ownership per service (avoids double counting):",
          "tableMetric": "Metric",
          "tableOwner": "Owner",
          "tableLabels": "Labels",
          "ownershipNote": "Ownership Table: Each metric has a single service responsible for emitting it, avoiding double counting in distributed systems.",
          "ownershipDesc": ": Each metric has a single service responsible for emitting it, avoiding double counting in distributed systems."
        },
        "tracing": {
          "title": "üîç Distributed Tracing (OpenTelemetry)",
          "description": "End-to-end tracing with context propagation between services:",
          "propagatedVia": "trace_id: a1b2c3d4... (propagated via headers)",
          "autoInstrumentation": "Automatic Instrumentation",
          "fastapiRequests": "‚Ä¢ FastAPI requests/responses",
          "httpxCalls": "‚Ä¢ HTTPX client calls",
          "sqlalchemyQueries": "‚Ä¢ SQLAlchemy queries",
          "redisOps": "‚Ä¢ Redis operations",
          "customSpans": "Custom Spans",
          "langGraphNode": "‚Ä¢ LangGraph node execution",
          "ragDecisions": "‚Ä¢ RAG search decisions",
          "bookingFsm": "‚Ä¢ Booking FSM transitions",
          "llmCalls": "‚Ä¢ LLM API calls + tokens"
        },
        "tokenUsage": {
          "title": "üí∞ Token Usage & Cost Tracking",
          "description": "Token usage reports via Jaeger traces for cost control:",
          "dataExtracted": "Data extracted from OpenTelemetry spans with attributes"
        },
        "stateSnapshot": {
          "title": "üì∏ State Snapshot Telemetry",
          "description": "Each response includes a state snapshot for conversation debugging:"
        },
        "ragTelemetry": {
          "title": "üéØ RAG Decision Telemetry",
          "description": "Explainability for pricing/RAG pipeline decisions:",
          "allowsIdentify": "Allows identifying: incomplete catalog, missing aliases, inadequate score threshold."
        },
        "usageMonitoring": {
          "title": "üì° Usage Monitoring Middleware",
          "description": "Endpoint monitoring to support deprecation decisions:",
          "nonIntrusive": "Non-intrusive",
          "nonIntrusiveDesc": ": Doesn't block requests, only observes",
          "sampleRate": "Sample rate",
          "sampleRateDesc": ": Configurable per environment (prod: 1%, dev: 100%)",
          "thirtyDaysData": "30+ days data",
          "thirtyDaysDesc": ": Collects data before removal decisions",
          "endpointTracking": "Endpoint tracking",
          "endpointTrackingDesc": ": Identifies legacy endpoints still in use"
        },
        "performanceThresholds": {
          "title": "üìà Performance Thresholds",
          "description": "Automatic thresholds for alerts:",
          "warning": "‚ö†Ô∏è Warning",
          "critical": "üö® Critical"
        },
        "title": "Complete Stack"
      },
      "architecture": {
        "stackTitle": "OBSERVABILITY STACK",
        "orchestrator": "Orchestrator",
        "aiEngine": "AI Engine",
        "memoryEngine": "Memory Engine",
        "metricsLabel": "metrics",
        "prometheusSection": "PROMETHEUS",
        "prometheusLine1": "- Business metrics (handover, booking, AI)",
        "prometheusLine2": "- System metrics (latency, errors, cache)",
        "prometheusLine3": "- Custom metrics (RAG, tool budget)",
        "grafanaSection": "GRAFANA DASHBOARDS",
        "grafanaLine1": "- Real-time KPIs per tenant",
        "grafanaLine2": "- Cost tracking & forecasting",
        "grafanaLine3": "- Alert rules & notifications",
        "otelSection": "OPENTELEMETRY + JAEGER",
        "otelLine1": "- Distributed traces across services",
        "otelLine2": "- Token usage extraction from spans",
        "otelLine3": "- Latency breakdown per operation",
        "lokiSection": "LOKI + JSON LOGS",
        "lokiLine1": "- Structured logging with trace correlation",
        "lokiLine2": "- trace_id + span_id in every log entry",
        "lokiLine3": "- Searchable by tenant, operation, error",
        "title": "Architecture"
      },
      "decisions": {
        "ownershipQ": "Why an ownership table for metrics?",
        "ownershipA": "In distributed systems, multiple services can observe the same event. Without clear ownership, handover metrics would be emitted by the Orchestrator AND the AI Engine, causing double counting. The ownership table defines a single emitter per metric.",
        "tracesQ": "Why extract tokens from traces and not logs?",
        "tracesA": "Traces have a standardized structure (span attributes) and context (trace_id links request to response). Logs are free text that require parsing. Jaeger already indexes spans by attribute, making queries like \"total tokens per model in the last month\" easy.",
        "snapshotQ": "Why include a state snapshot in response_metadata?",
        "snapshotA": "Conversation debugging requires knowing the exact state after each turn. Including a snapshot in the response lets you reproduce issues without accessing logs or traces. The frontend can show \"internal state\" in debug mode.",
        "sampleRateQ": "Why configurable sample rate?",
        "sampleRateA": "In production with millions of requests, 100% sampling is expensive (storage, processing). 1-10% sampling captures enough anomalies for debugging. In dev/staging, 100% enables full debugging of every request.",
        "title": "Technical Decisions"
      },
      "techStack": {
        "title": "Technical Stack"
      },
      "cta": {
        "next": "Next: Backend Orchestrator ‚Üí"
      }
    },
    "optimus": {
      "back": "‚Üê Back",
      "title": "Optimus",
      "subtitle": "AI Chatbot Platform for Millions of Users",
      "problem": {
        "title": "The Problem A company needed a chatbot platform that could serve clinics, e-commerce, law offices ‚Äî any business vertical. The system had to scale to millions of users, keep long-conversation context, and fully isolate data between clients. Generic chatbots don't cut it. They forget what you said 5 messages ago and don't understand that \"tooth 36\" in a clinic is critical information that cannot be lost.",
        "p1": "The Problem A company needed a chatbot platform that could serve clinics, e-commerce, law offices ‚Äî any business vertical. The system had to scale to millions of users, keep long-conversation context, and fully isolate data between clients. Generic chatbots don't cut it. They forget what you said 5 messages ago and don't understand that \"tooth 36\" in a clinic is critical information that cannot be lost.",
        "p2": "The Problem A company needed a chatbot platform that could serve clinics, e-commerce, law offices ‚Äî any business vertical. The system had to scale to millions of users, keep long-conversation context, and fully isolate data between clients. Generic chatbots don't cut it. They forget what you said 5 messages ago and don't understand that \"tooth 36\" in a clinic is critical information that cannot be lost."
      },
      "solution": {
        "title": "The Solution",
        "intro": "A microservices architecture from scratch, designed for production from day one:",
        "orchestrator": "Single gateway. Rate limiting, cache, circuit breaker.",
        "aiEngine": "Processes conversations with LangChain. Zero database access.",
        "memoryEngine": "Owner of PostgreSQL. Hierarchical memory (hot/warm/cold).",
        "rulesEngine": "Business automations per vertical."
      },
      "multiTenancy": {
        "title": "True Multi-Tenancy",
        "p1": "Each client has its own PostgreSQL schema ({clinicExample}, {storeExample}). It's not just a tenant_id in a shared table ‚Äî it's real isolation. A bug in one tenant's code doesn't leak another's data."
      },
      "forgetting": {
        "title": "The \"Forgetting\" Problem Chatbots lose context. After 50 messages, they forget you booked an appointment for Tuesday at 2pm. I solved it with three-layer memory:",
        "p1": "The \"Forgetting\" Problem Chatbots lose context. After 50 messages, they forget you booked an appointment for Tuesday at 2pm. I solved it with three-layer memory:",
        "p2": "The \"Forgetting\" Problem Chatbots lose context. After 50 messages, they forget you booked an appointment for Tuesday at 2pm. I solved it with three-layer memory:",
        "hot": "Redis, latest messages, instant access",
        "warm": "Redis, recent context, longer TTL",
        "cold": "PostgreSQL, full history",
        "p3": "When a human operator hands back to the bot, the system generates a summary via GPT-4 with templates specific to each vertical. In a clinic, it knows that \"tooth 36\" and \"amoxicillin 500mg\" are critical."
      },
      "numbers": {
        "title": "Numbers",
        "latency": "P95 latency",
        "cacheHit": "Cache hit",
        "uptime": "Uptime",
        "throughput": "Throughput"
      },
      "stack": {
        "title": "Stack"
      },
      "learnings": {
        "title": "What I learned",
        "p1Label": "Separation of responsibilities isn't fluff.",
        "p1": "After 6 months, when you need to change the cache without breaking the AI Engine, you'll be grateful.",
        "p2Label": "Multi-tenancy must be thought of from day one.",
        "p2": "Per-tenant schema in PostgreSQL is more upfront work, but you sleep well.",
        "p3Label": "Chatbot memory is underestimated.",
        "p3": "Everyone focuses on the model. Nobody talks about what happens after 200 messages."
      },
      "cta": {
        "interested": "Interested in building something similar?",
        "letsChat": "Let's talk"
      }
    },
    "aiEngine": {
      "back": "Back",
      "platformName": "Optimus AI Platform",
      "badge1": "Core Service",
      "badge2": "AI Architecture",
      "title": "AI Conversation",
      "titleHighlight": "Engine",
      "subtitle": "Enterprise conversation engine that separates deterministic logic from natural language. LangGraph orchestrates 20+ specialized nodes, an 8-state FSM guarantees critical behaviors, and the Semantic Router decides in &lt;10ms when the LLM can be bypassed.",
      "stats": {
        "nodes": "LangGraph Nodes",
        "states": "FSM States",
        "routing": "Semantic Routing",
        "uptime": "Uptime"
      },
      "nav": {
        "problem": "Problem",
        "architecture": "Architecture",
        "semanticRouting": "Semantic Routing",
        "fsm": "Booking FSM",
        "contextComposer": "Context Composer",
        "fastLane": "Pricing Fast Lane",
        "reducer": "Flow Reducer",
        "retry": "Retry Policies",
        "results": "Results"
      },
      "sections": {
        "problem": {
          "title": "The Problem",
          "content1": "Chatbots based purely on LLMs suffer from a fundamental problem:",
          "nonDeterminism": "non-determinism",
          "scenariosTitle": "Unacceptable Scenarios",
          "scenarios": {
            "hallucination": {
              "title": "\"Schedule hallucination\":",
              "content": "LLM confirms an appointment at 2pm when the user said 3pm. Patient misses appointment, clinic loses revenue."
            },
            "ambiguous": {
              "title": "\"Ambiguous confirmation\":",
              "content": "User says \"ok\", LLM interprets as confirmation. But it was \"ok, got it\" (acknowledgement), not \"ok, confirm\"."
            },
            "drift": {
              "title": "\"Intent drift\":",
              "content": "A conversation about price turns into scheduling without the user asking. LLM \"helping too much\"."
            },
            "loop": {
              "title": "\"Infinite loop\":",
              "content": "LLM calls a tool, fails, calls again, fails... until timeout or cost explodes."
            }
          },
          "conclusion": "For an enterprise medical/dental scheduling system, this is unacceptable. The solution isn't \"better prompting\" ‚Äî it's separating what must be deterministic (business decisions) from what can be probabilistic (natural language generation).",
          "content2": ". The same input can generate different outputs, and critical business decisions are at the mercy of \"temperature\" and random context."
        },
        "architecture": {
          "title": "Hybrid Architecture",
          "intro1": "The AI Engine implements a hybrid architecture where the",
          "introHighlight2": "tasks where creativity is desirable",
          "diagramTitle": "Execution Flow",
          "langGraphTitle": "LangGraph: Why not simple chains?",
          "langGraphExplanation": "Chains are linear ‚Äî input ‚Üí processing ‚Üí output. Booking flow needs complex branching: a user can cancel in the middle of a reschedule, ask for clarification, or escalate to a human at any time. LangGraph allows modeling this as a real graph.",
          "intro2": "(FSM + rules), and the LLM is used only for",
          "intro3": "(humanization, clarification).",
          "introHighlight1": "business logic is 100% deterministic"
        },
        "semanticRouting": {
          "title": "Semantic Routing",
          "intro": "Before calling the LLM for structured analysis (expensive, ~200ms), the Semantic Router tries to classify intent via embeddings (cheap, &lt;10ms). If confidence is high, it bypasses the LLM completely.",
          "hybridAlphaTitle": "Hybrid Alpha: Embeddings + Keywords",
          "hybridAlphaExplanation": "Pure embeddings fail in cases where keywords are critical (e.g., \"cancel\" vs \"I'd like to reschedule\"). The Semantic Router uses hybrid classification:",
          "hybridAlphaConfig": "Configurable via env:",
          "params": {
            "hybridAlpha": {
              "title": "HYBRID_ALPHA",
              "description": "0.7 = 70% embeddings. Lower values prioritize keywords."
            },
            "abstainThreshold": {
              "title": "ABSTAIN_THRESHOLD",
              "description": "0.55 = minimum confidence. Below that, it goes to the LLM."
            },
            "biasWeight": {
              "title": "BIAS_WEIGHT",
              "description": "0.03 = boost for critical intents (booking > greeting)."
            }
          },
          "resultTitle": "‚úì Result",
          "resultContent": "~40% of requests are classified with high confidence and bypass the LLM completely. Savings of ~$0.002/request √ó 40% = significant OpenAI cost reduction."
        },
        "fsm": {
          "title": "Booking State Machine",
          "intro": "The heart of the booking flow is an FSM (Finite State Machine) that defines exactly which transitions are valid. It doesn't matter what the LLM \"thinks\" ‚Äî if the transition isn't in the table, it doesn't happen.",
          "diagramTitle": "States and Transitions",
          "transitionsTitle": "Valid Transitions (SSoT)",
          "operationsTitle": "Supported Operations",
          "operations": {
            "schedule": {
              "title": "schedule",
              "description": "New booking. Find slots ‚Üí selection ‚Üí confirmation ‚Üí ERP create."
            },
            "reschedule": {
              "title": "reschedule",
              "description": "Reschedule existing. Find booking ‚Üí slots ‚Üí confirmation ‚Üí ERP update."
            },
            "cancel": {
              "title": "cancel",
              "description": "Cancel existing. Find booking ‚Üí confirmation ‚Üí ERP delete."
            }
          }
        },
        "contextComposer": {
          "title": "Context Composer (ECA)",
          "intro": "Context Composer implements Enhanced Context Architecture (ECA) ‚Äî a deterministic context assembly system that pulls data from multiple sources, applies a token budget, and formats ordered blocks for the LLM.",
          "budgetTitle": "Budget Management",
          "budgetIntro": "With 1200-token context and multiple sources, it's easy to blow the budget. Context Composer uses smart truncation:",
          "budgetRules": {
            "untouchable": {
              "title": "Untouchable blocks:",
              "content": "IDENTITY, RULES, INPUT are never truncated"
            },
            "semantic": {
              "title": "Semantic compression:",
              "content": "MEMORY can be compressed (90% reduction via Memory Engine)"
            },
            "priority": {
              "title": "Priority truncation:",
              "content": "FOCUS (RAG) is truncated first if needed"
            }
          }
        },
        "fastLane": {
          "title": "Pricing Fast Lane",
          "intro": "Price inquiries are the most common use case (~35% of messages) and have a predictable pattern: RAG lookup + formatting. Pricing Fast Lane executes this directly, without going through agent_node (which runs expensive LLM iterations).",
          "resultTitle": "‚ö° Result",
          "resultContent": "Fast lane reduces latency from ~2s (agent iteration) to ~200ms (direct RAG). Cache hit: &lt;50ms. No LLM token cost for ~35% of requests."
        },
        "reducer": {
          "title": "Booking Flow Reducer",
          "intro": "Reducer is the guardian of booking_flow state. Every update goes through it, validating source whitelist, turn_seq monotonicity, and valid transitions. If something violates the invariants, the update is rejected.",
          "whyReducerTitle": "Why Reducer?",
          "reasons": {
            "raceConditions": {
              "title": "Prevents race conditions:",
              "content": "Multiple nodes may try to update state simultaneously. Reducer serializes and validates."
            },
            "audit": {
              "title": "Audit:",
              "content": "Each update is logged with source, turn_seq, and transition. Easier debugging."
            },
            "metrics": {
              "title": "Metrics:",
              "content": "Prometheus counters for updates and rejections by reason."
            }
          }
        },
        "retry": {
          "title": "Retry Policies",
          "intro": "Not every error deserves a retry. Transient errors (timeout, rate limit) are retriable. Permanent errors (auth failure, bad request) are not ‚Äî retries only waste time and money.",
          "warningTitle": "‚ö†Ô∏è Why whitelist and not blacklist?",
          "warningContent": "Blacklist is dangerous: if a new type of permanent error appears (e.g., a new OpenAI SDK exception), it would be retried by default. Whitelist is fail-safe: only retry what we know is transient."
        },
        "results": {
          "title": "Results",
          "metrics": {
            "bypass": {
              "value": "~40%",
              "label": "Requests bypass LLM via Semantic Routing"
            },
            "classification": {
              "value": "&lt;10ms",
              "label": "Classification via embeddings (p95)"
            },
            "transitions": {
              "value": "0",
              "label": "Invalid FSM transitions in production"
            },
            "fastLane": {
              "value": "~200ms",
              "label": "Pricing Fast Lane (vs ~2s agent)"
            }
          },
          "decisionsTitle": "Key Technical Decisions",
          "decisions": {
            "fsmSeparated": {
              "title": "FSM separated from the LLM:",
              "content": "Business decisions are deterministic. The LLM only humanizes."
            },
            "semanticPreLlm": {
              "title": "Semantic routing pre-LLM:",
              "content": "Cheap classification before spending tokens."
            },
            "reducerWhitelist": {
              "title": "Reducer with whitelist:",
              "content": "Only authorized sources update critical state."
            },
            "retryWhitelist": {
              "title": "Retry with whitelist:",
              "content": "Only transient exceptions are retried."
            },
            "fastLanes": {
              "title": "Fast lanes for known patterns:",
              "content": "Pricing doesn't need LLM iteration."
            }
          }
        }
      },
      "techStack": {
        "title": "Technical Stack"
      },
      "cta": {
        "title": "Explore Other Case Studies",
        "description": "See how other Optimus components were built",
        "rulesEngine": "Rules Engine ‚Üí",
        "memoryEngine": "Memory Engine ‚Üí",
        "llmPool": "LLM Pool Management ‚Üí"
      },
      "footer": "Case Study: AI Conversation Engine ‚Äî Optimus AI Platform"
    },
    "mcpServers": {
      "tag": "Developer Tools",
      "title": "MCP Servers",
      "description": "Custom Model Context Protocol servers for development with Claude Code. Debugging tools, architectural validation, and intelligent codebase navigation.",
      "whatIsMcp": {
        "title": "What is MCP?",
        "intro": "<strong class=\"text-white\">Model Context Protocol</strong> is an Anthropic standard that lets you extend AI assistants (like Claude) with custom tools. Instead of the AI \"guessing\" about your codebase, you give it real access to your architecture, contracts, and conventions.",
        "comment": "# Add MCP server to Claude Code"
      },
      "servers": {
        "title": "Available Servers",
        "orchestrator": {
          "title": "backend_orchestrator_mcp",
          "intro": "Debugging tools for Backend Orchestrator:",
          "diagnoseGateway": "Full gateway health check",
          "analyzeCache": "PubSub cache performance",
          "analyzeRateLimiting": "Rate limiting status per tenant",
          "generateRouter": "Router template following standards"
        },
        "optimus": {
          "title": "optimus_project_mcp",
          "intro": "12 tools for intelligent codebase navigation:",
          "systemOverview": "Architecture overview",
          "serviceContract": "Contracts for each microservice",
          "validateDiff": "Validate changes against guardrails",
          "planChange": "Architectural guidance for changes"
        }
      },
      "validation": {
        "title": "Architectural Validation",
        "intro": "The <code class=\"text-cyan-400\">validate_diff checks whether a diff respects the project's architectural guardrails:",
        "violationDetected": "‚ùå Violation Detected ai-engine/src/service.py: Line 42 \"import sqlalchemy\" ‚Üê AI Engine cannot access DB directly! Guardrail: \"All data operations must go through Memory Engine APIs\"",
        "violationExample": "‚ùå Violation Detected ai-engine/src/service.py: Line 42 \"import sqlalchemy\" ‚Üê AI Engine cannot access DB directly! Guardrail: \"All data operations must go through Memory Engine APIs\"",
        "validDiff": "‚úÖ Valid Diff ai-engine/src/service.py: Line 42 \"await memory_engine_client.get_data(...)\" ‚úì Respects Memory Engine API boundary",
        "validExample": "‚úÖ Valid Diff ai-engine/src/service.py: Line 42 \"await memory_engine_client.get_data(...)\" ‚úì Respects Memory Engine API boundary"
      },
      "hotReload": {
        "title": "Hot Reload",
        "intro": "Modules are reloaded without restarting the MCP server:"
      },
      "stack": {
        "title": "Technical Stack"
      },
      "cta": {
        "next": "Next: Feed-RSS Monitor ‚Üí"
      }
    },
    "feedRss": {
      "tag": "Automation",
      "title": "Feed-RSS Monitor",
      "description": "Content automation pipeline that monitors RSS feeds, filters by keywords, generates scripts via OpenAI, and notifies in Telegram/Discord.",
      "stats": {
        "monitoredFeeds": "Monitored Feeds",
        "aiKeywords": "AI Keywords",
        "asyncPipeline": "Python Pipeline"
      },
      "overview": {
        "title": "Overview",
        "intro": "Solution for content creators who need to track AI/tech news and quickly turn them into social media content.",
        "input": "Input",
        "inputDesc": "TechCrunch, The Verge, Wired, Ars Technica, MIT Tech Review, Engadget",
        "output": "Output",
        "outputDesc": "Scripts for YouTube Shorts (50s) delivered via Telegram/Discord"
      },
      "architecture": {
        "title": "Architecture"
      },
      "services": {
        "title": "Modular Architecture",
        "feedService": {
          "title": "FeedService",
          "desc": "Async feed fetching with aiohttp. Configurable timeout, retry on failures, normalization of entries into a Pydantic model."
        },
        "filterService": {
          "title": "FilterService",
          "desc": "Filters news by keywords (case-insensitive). Keywords configurable via .env or external file. Combines title + summary for matching."
        },
        "aiService": {
          "title": "AIService",
          "desc": "OpenAI integration with a prompt optimized for Shorts scripts. Structured format: impactful hook ‚Üí 3-5 core sentences ‚Üí CTA with a question."
        },
        "stateService": {
          "title": "StateService",
          "desc": "Persists IDs of already processed news in JSON. Avoids reprocessing and spam from repeated notifications."
        }
      },
      "prompt": {
        "title": "Prompt Engineering",
        "intro": "The prompt was optimized to generate concise and impactful scripts:",
        "systemPrompt": "System Prompt:",
        "promptText": "\"You are a writer for the Cyber Inteligente channel. Create scripts for YouTube Shorts up to 50 seconds, in clear and direct English. Energetic, futuristic tone, no fluff. Format: (1) Hook in 1 sentence; (2) Essence in 3-5 short sentences; (3) CTA with a question.\""
      },
      "usage": {
        "title": "Usage",
        "basicExecution": "# Basic execution",
        "withOptions": "# With options"
      },
      "stack": {
        "title": "Technical Stack"
      },
      "cta": {
        "backToHome": "‚Üê Back to Home"
      }
    },
    "common": {
      "personalProject": "Projeto Pessoal",
      "technicalStack": "Stack T√©cnico",
      "results": "Resultados"
    },
    "pvcoach": {
      "tag": "Chess + AI",
      "tagSecondary": "Training Tool",
      "title": "PVCoach",
      "description": "Chess coach that combines Stockfish with an LLM to explain moves. MultiPV analysis, progressive hints, explanations grounded in engine variations, and automatic validation to avoid hallucinations.",
      "stats": {
        "llmProviders": "LLM Providers",
        "topNAnalysis": "Top-N Analysis",
        "hintLevels": "Hint Levels",
        "groundedExplanations": "Explanations"
      },
      "nav": {
        "problem": "Problem",
        "architecture": "Architecture",
        "multipv": "MultiPV",
        "grounded": "Grounded Explanations",
        "hints": "Progressive Hints",
        "providers": "Multi-Provider",
        "api": "API"
      },
      "problem": {
        "title": "The Problem",
        "intro": "Chess engines like Stockfish are extremely strong, but their \"explanations\" are just numbers (centipawns) and raw variations. Intermediate players can't understand <em>why</em> a move is better.",
        "gapTitle": "The Understanding Gap",
        "engineSays": "Engine says:",
        "engineExample": "\"e4 +0.35, d4 +0.20\" ‚Äî OK, but why?",
        "llmHallucinates": "Pure LLM hallucinates:",
        "llmExample": "\"Nf3 attacks the queen\" ‚Äî No, it doesn't.",
        "humanExpensive": "Human analysis is expensive:",
        "humanExample": "Coaches charge $50-100/hour.",
        "solution": "The solution was to combine Stockfish's precision with the LLM's explanatory ability, but <strong class=\"text-white\">grounding explanations in the engine's real variations</strong> to avoid hallucinations."
      },
      "architecture": {
        "title": "Architecture",
        "pipelineTitle": "Analysis Pipeline",
        "fenPosition": "FEN Position",
        "stockfishMultipv": "Stockfish MultiPV",
        "depthMultipv": "depth=14, multipv=3",
        "llmGrounded": "LLM (Grounded Prompt)",
        "explanationPv": "Explanation based on PVs",
        "validation": "Validation",
        "crossCheck": "Cross-check with engine"
      },
      "multipv": {
        "title": "MultiPV Analysis",
        "intro": "Instead of asking for only the \"best move\", the system asks for the top-N moves with their full variations. This allows comparing alternatives and explaining <em>why</em> one move is better than another.",
        "deltaExplanation": "The <code>delta_cp</code> shows how much each alternative \"loses\" relative to the best move. Ex: if e4 = +35cp and d4 = +20cp, then d4's delta_cp = 15cp (loses 0.15 pawns)."
      },
      "grounded": {
        "title": "Grounded Explanations",
        "intro": "The secret to avoid hallucinations is <strong class=\"text-white\">grounding the LLM in the engine's real variations</strong>. The prompt includes the formatted PVs, and validation checks whether the explanation mentions moves that actually exist.",
        "validationTitle": "Automatic Validation",
        "resultTitle": "Result",
        "resultText": "If the LLM mentions a move that doesn't exist in the PVs, the system returns a warning. This allows detecting hallucinations before showing the user."
      },
      "hints": {
        "title": "Progressive Hints",
        "intro": "For training, revealing the move immediately doesn't help. The system offers progressive hints in 3 levels, from the vaguest to the most specific.",
        "level1Title": "Level 1: Hint",
        "level1Example": "\"Think about moving your knight.\"",
        "level1Desc": "Indicates the piece, not the square. If there's a capture or check, it adds a note.",
        "level2Title": "Level 2: Plan",
        "level2Example": "\"Best move: Nf3. Compare with d4, Nc3.\"",
        "level2Desc": "Reveals the move + alternatives + delta_cp.",
        "level3Title": "Level 3: Variation",
        "level3Example": "\"Nf3 ‚Üí d5 d4 Nf6 c4 e6\"",
        "level3Desc": "Full PV + LLM explanation (optional)."
      },
      "providers": {
        "title": "Multi-Provider LLM",
        "intro": "The system supports 5 LLM providers, allowing you to choose by cost, speed, or preference. Configurable via env vars or per-request.",
        "openai": "GPT-4, GPT-3.5 ‚Äî Chat Completions API.",
        "anthropic": "Claude 3.5 Sonnet, Haiku ‚Äî Messages API.",
        "google": "Gemini 1.5 Flash, Pro ‚Äî GenerativeAI.",
        "openrouter": "Proxy for multiple models ‚Äî OpenAI-compatible."
      },
      "api": {
        "title": "API Endpoints",
        "evaluatePosition": "Pure MultiPV evaluation (no LLM). Returns top-N candidates with PVs and scores.",
        "explainPosition": "Evaluation + grounded LLM explanation. Includes validation warnings.",
        "hintsPosition": "Progressive hints (levels 1-3). Level 3 may include explanation.",
        "analyze": "Downloads games from Lichess and analyzes in batch."
      },
      "results": {
        "hallucinations": "Hallucinations with active validation",
        "providers": "Supported LLM providers",
        "cache": "TTL for evaluations and explanations",
        "hintLevels": "Progressive hint levels"
      },
      "cta": {
        "title": "Explore Other Projects",
        "subtitle": "See other systems I built"
      },
      "footer": "Case Study: PVCoach ‚Äî Chess Training with AI"
    },
    "documentProcessing": {
      "tag": "Document AI",
      "title": "Document Processing Pipeline",
      "description": "Pipeline de processamento de documentos com detec√ß√£o autom√°tica de tabelas de pre√ßo, chunking estrat√©gico e gera√ß√£o autom√°tica de cat√°logos para RAG.",
      "stats": {
        "autoDetect": "Auto-detect Tables",
        "brokenChunks": "Broken Price Chunks",
        "indexUpdate": "Index Update",
        "multiTenant": "Tenant Isolated"
      },
      "problem": {
        "title": "O Problema",
        "intro": "Cl√≠nicas enviam tabelas de pre√ßo em formatos diversos (PDF, Excel, Word). Chunking tradicional quebra tabelas no meio, destruindo a rela√ß√£o entre procedimento e pre√ßo:",
        "brokenChunk": "‚ùå Chunk quebrado no meio da tabela:",
        "chunkEnd": "--- FIM DO CHUNK ---"
      },
      "solution": {
        "title": "A Solu√ß√£o",
        "docling": {
          "title": "Detec√ß√£o Autom√°tica de Tabelas (Docling)",
          "intro": "An√°lise estrutural do documento para identificar tabelas de pre√ßo vs texto corrido (FAQ, pol√≠ticas):",
          "layout": "Heur√≠sticas de layout (colunas, bordas, alinhamento)",
          "currency": "Detec√ß√£o de padr√µes monet√°rios (R$, $, ‚Ç¨)",
          "headers": "Headers t√≠picos (\"Procedimento\", \"Valor\", \"Pre√ßo\")"
        },
        "chunking": {
          "title": "Chunking Estrat√©gico",
          "intro": "Estrat√©gias diferentes para tipos de conte√∫do diferentes:",
          "tables": {
            "title": "üìä Tabelas de Pre√ßo TableRow Strategy: Cada linha da tabela vira um chunk at√¥mico. Nunca quebra no meio de um pre√ßo.",
            "desc": "<p class=\"text-gray-500 text-sm\">\n<strong class=\"text-white\">TableRow Strategy:</strong> Cada linha da tabela \nvira um chunk at√¥mico. Nunca quebra no meio de um pre√ßo."
          },
          "faq": {
            "title": "üìù FAQ / Pol√≠ticas Semantic Chunking: Chunks por par√°grafo ou se√ß√£o sem√¢ntica, com overlap para contexto.",
            "desc": "<p class=\"text-gray-500 text-sm\">\n<strong class=\"text-white\">Semantic Chunking:</strong> Chunks por par√°grafo \nou se√ß√£o sem√¢ntica, com overlap para contexto."
          }
        },
        "catalog": {
          "title": "Cat√°logo Autom√°tico",
          "intro": "Extra√ß√£o estruturada gera cat√°logo JSON pronto para busca:"
        }
      },
      "pipeline": {
        "title": "Pipeline de Processamento"
      },
      "hnsw": {
        "title": "HNSW Index Auto-Update",
        "intro": "√çndices HNSW s√£o atualizados automaticamente via Celery workers, garantindo que novos documentos estejam dispon√≠veis para busca em minutos:",
        "trigger": "Trigger: Document upload or scheduled run",
        "rebuild": "Rebuild: Incremental when possible, full rebuild if needed",
        "zeroDowntime": "Zero-downtime: New index created in parallel, atomic swap"
      },
      "stack": {
        "title": "Stack T√©cnica"
      },
      "cta": {
        "next": "Pr√≥ximo: MCP Servers ‚Üí"
      }
    }
  }
}
